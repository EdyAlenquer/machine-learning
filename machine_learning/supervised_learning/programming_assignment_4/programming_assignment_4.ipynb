{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment 4\n",
    "## Neural Networks\n",
    "\n",
    "Aluno: Francisco Edyvalberty Alenquer Cordeiro \\\n",
    "MatrÃ­cula: 518659\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    y_true = y_true.reshape(-1, 1)\n",
    "    y_pred = y_pred.reshape(-1, 1)\n",
    "\n",
    "    right_prediction = y_true == y_pred\n",
    "    accuracy = right_prediction.sum() / len(y_true)\n",
    "    return accuracy\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    y_true = y_true.reshape(-1, 1)\n",
    "    y_pred = y_pred.reshape(-1, 1)\n",
    "    array = np.hstack([y_true, y_pred])\n",
    "    array = array[array[:,0] == 1]\n",
    "    \n",
    "    right_prediction = array[:, 0] == array[:, 1]\n",
    "    recall = right_prediction.sum() / len(array)\n",
    "    return recall\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    y_true = y_true.reshape(-1, 1)\n",
    "    y_pred = y_pred.reshape(-1, 1)\n",
    "    array = np.hstack([y_true, y_pred])\n",
    "    array = array[array[:,1] == 1]\n",
    "    \n",
    "    right_prediction = array[:, 0] == array[:, 1]\n",
    "    precision = right_prediction.sum() / len(array)\n",
    "\n",
    "    return precision\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    y_true = y_true.reshape(-1, 1)\n",
    "    y_pred = y_pred.reshape(-1, 1)\n",
    "    precision_score = precision(y_true, y_pred)\n",
    "    recall_score = recall(y_true, y_pred)\n",
    "\n",
    "    f1_score = 2 * (precision_score * recall_score) / (precision_score + recall_score)\n",
    "\n",
    "    return f1_score\n",
    "\n",
    "def get_mse(y_real, y_pred):\n",
    "    return np.mean((y_real - y_pred) ** 2)\n",
    "\n",
    "def get_rmse(y_real, y_pred):\n",
    "    return np.sqrt(get_mse(y_real, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinMaxScaler:\n",
    "    def __init__(self):\n",
    "        self.fitted = False\n",
    "\n",
    "    def fit_transform(self, data):      \n",
    "        self.maximum = data.max(axis=0)\n",
    "        self.minimum = data.min(axis=0)\n",
    "        self.fitted = True\n",
    "\n",
    "        scaled_data =  (data - self.minimum) / (self.maximum - self.minimum)\n",
    "        return scaled_data\n",
    "    \n",
    "    def transform(self, data):\n",
    "        if not self.fitted:\n",
    "            raise Exception('Scaler not fitted!')\n",
    "\n",
    "        scaled_data =  (data - self.minimum) / (self.maximum - self.minimum)\n",
    "        return scaled_data\n",
    "\n",
    "    def inverse_transform(self, scaled_data):\n",
    "        if not self.fitted:\n",
    "            raise Exception('Scaler not fitted!')\n",
    "        \n",
    "        original_data = (self.maximum - self.minimum) * scaled_data + self.minimum\n",
    "        return original_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardScaler:\n",
    "    def __init__(self):\n",
    "        self.fitted = False\n",
    "\n",
    "    def fit_transform(self, data):\n",
    "        self.mean = data.mean(axis=0)\n",
    "        self.std = data.std(axis=0)\n",
    "        self.fitted = True\n",
    "\n",
    "        scaled_data = (data - self.mean) / self.std\n",
    "        return scaled_data\n",
    "\n",
    "    def transform(self, data):\n",
    "        if not self.fitted:\n",
    "            raise Exception('Scaler not fitted!')\n",
    "\n",
    "        scaled_data = (data - self.mean) / self.std\n",
    "        return scaled_data\n",
    "\n",
    "    def inverse_transform(self, scaled_data):\n",
    "        if not self.fitted:\n",
    "            raise Exception('Scaler not fitted!')\n",
    "\n",
    "        original_data = (scaled_data * self.std) + self.mean\n",
    "        return original_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cv_folds(data, n_folds=10, shuffle=True, random_state=12894):\n",
    "    indexes = np.arange(data.shape[0])\n",
    "    if shuffle:\n",
    "        np.random.seed(random_state)\n",
    "        np.random.shuffle(indexes)\n",
    "\n",
    "    slices = np.array_split(indexes, n_folds)\n",
    "    all_elements = np.hstack(slices)   \n",
    "    \n",
    "    splits = []\n",
    "    for i in range(n_folds):\n",
    "        train_idx = all_elements[~np.isin(all_elements, slices[i])]\n",
    "        test_idx = slices[i]\n",
    "\n",
    "        splits.append((train_idx, test_idx))\n",
    "\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, y, train_size_perc, random_seed=264852):\n",
    "    \n",
    "    y = y.reshape(-1, 1)\n",
    "    \n",
    "    N = X.shape[0]\n",
    "    train_size = int(train_size_perc * N)\n",
    "\n",
    "    indexes = np.arange(0, N, 1)\n",
    "\n",
    "    np.random.seed(random_seed)\n",
    "    train_idx = np.random.choice(indexes, train_size, replace=False)\n",
    "    test_idx = np.delete(indexes, train_idx)\n",
    "\n",
    "    X_train = X[train_idx, :]\n",
    "    y_train = y[train_idx, :]\n",
    "    X_test = X[test_idx, :]\n",
    "    y_test = y[test_idx, :]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do Cross Validation and Get Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_cv_and_get_metrics(classifier, cv_splits, X_train, y_train, X_test, title='Classifier', scaler=None):\n",
    "\n",
    "    X_train = X_train.copy()\n",
    "    y_train = y_train.copy()\n",
    "    X_test = X_test.copy()\n",
    "\n",
    "    train_metrics = {\n",
    "        'accuracy': [],\n",
    "        'recall': [],\n",
    "        'precision': [],\n",
    "        'f1_score': []\n",
    "    }\n",
    "\n",
    "    valid_metrics = {\n",
    "        'accuracy': [],\n",
    "        'recall': [],\n",
    "        'precision': [],\n",
    "        'f1_score': []\n",
    "    }\n",
    "    # Reporting results\n",
    "    print('#' + f'{title}'.center(60, '-') + '#')\n",
    "\n",
    "    print('\\n---> Validation Folds Metrics')\n",
    "    print('Fold\\tAccuracy\\tRecall\\t\\tPrecision\\tF1-Score')\n",
    "    count_fold = 1\n",
    "    for train_idx, val_idx in cv_splits:\n",
    "        # Spliting data\n",
    "        X_train_cv = X_train[train_idx, :]\n",
    "        y_train_cv = y_train[train_idx, :]\n",
    "        X_val_cv = X_train[val_idx, :]\n",
    "        y_val_cv = y_train[val_idx, :]\n",
    "\n",
    "        # Scaling if have scaler argument\n",
    "        if scaler is not None:\n",
    "            X_train_cv = scaler.fit_transform(X_train_cv)\n",
    "            X_val_cv = scaler.transform(X_val_cv)\n",
    "\n",
    "        # Training Model\n",
    "        classifier.fit(X_train_cv, y_train_cv.ravel())\n",
    "\n",
    "        # Predictions\n",
    "        y_train_cv_pred = classifier.predict(X_train_cv)\n",
    "        y_val_cv_pred = classifier.predict(X_val_cv)\n",
    "\n",
    "        # Storing metrics\n",
    "        train_metrics['accuracy'].append(accuracy(y_train_cv, y_train_cv_pred))\n",
    "        train_metrics['recall'].append(recall(y_train_cv, y_train_cv_pred))\n",
    "        train_metrics['precision'].append(precision(y_train_cv, y_train_cv_pred))\n",
    "        train_metrics['f1_score'].append(f1_score(y_train_cv, y_train_cv_pred))\n",
    "\n",
    "        valid_metrics['accuracy'].append(accuracy(y_val_cv, y_val_cv_pred))\n",
    "        valid_metrics['recall'].append(recall(y_val_cv, y_val_cv_pred))\n",
    "        valid_metrics['precision'].append(precision(y_val_cv, y_val_cv_pred))\n",
    "        valid_metrics['f1_score'].append(f1_score(y_val_cv, y_val_cv_pred))\n",
    "\n",
    "        print('{0:.0f}\\t{1:.4f}  \\t{2:.4f}\\t\\t{3:.4f}   \\t{4:.4f}'.format(\n",
    "                count_fold,\n",
    "                valid_metrics['accuracy'][-1], \n",
    "                valid_metrics['recall'][-1],\n",
    "                valid_metrics['precision'][-1],\n",
    "                valid_metrics['f1_score'][-1]\n",
    "            )\n",
    "        )\n",
    "        count_fold+=1\n",
    "\n",
    "\n",
    "    print('\\n--->\\tTraining Metrics')\n",
    "\n",
    "    print('Accuracy Mean:     \\t{0:.4f} | Accuracy Std:   \\t{1:.4f}'.format(\n",
    "        np.mean(train_metrics['accuracy']), \n",
    "        np.std(train_metrics['accuracy']))\n",
    "    )\n",
    "    print('Recall Mean:     \\t{0:.4f} | Recall Std:       \\t{1:.4f}'.format(\n",
    "        np.mean(train_metrics['recall']), \n",
    "        np.std(train_metrics['recall']))\n",
    "    )\n",
    "    print('Precision Mean:     \\t{0:.4f} | Precision Std:   \\t{1:.4f}'.format(\n",
    "        np.mean(train_metrics['precision']), \n",
    "        np.std(train_metrics['precision']))\n",
    "    )\n",
    "    print('F1 Score Mean:     \\t{0:.4f} | F1 Score Std:   \\t{1:.4f}'.format(\n",
    "        np.mean(train_metrics['f1_score']), \n",
    "        np.std(train_metrics['f1_score']))\n",
    "    )\n",
    "\n",
    "    print('\\n--->\\tValidation Metrics')\n",
    "\n",
    "    print('Accuracy Mean:     \\t{0:.4f} | Accuracy Std:   \\t{1:.4f}'.format(\n",
    "        np.mean(valid_metrics['accuracy']), \n",
    "        np.std(valid_metrics['accuracy']))\n",
    "    )\n",
    "    print('Recall Mean:     \\t{0:.4f} | Recall Std:       \\t{1:.4f}'.format(\n",
    "        np.mean(valid_metrics['recall']), \n",
    "        np.std(valid_metrics['recall']))\n",
    "    )\n",
    "    print('Precision Mean:     \\t{0:.4f} | Precision Std:   \\t{1:.4f}'.format(\n",
    "        np.mean(valid_metrics['precision']), \n",
    "        np.std(valid_metrics['precision']))\n",
    "    )\n",
    "    print('F1 Score Mean:     \\t{0:.4f} | F1 Score Std:   \\t{1:.4f}'.format(\n",
    "        np.mean(valid_metrics['f1_score']), \n",
    "        np.std(valid_metrics['f1_score']))\n",
    "    )\n",
    "\n",
    "    print('\\n--->\\tTest Metrics')\n",
    "\n",
    "    if scaler is not None:\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "    \n",
    "    classifier.fit(X_train, y_train.ravel())\n",
    "    y_test_pred = classifier.predict(X_test)\n",
    "\n",
    "    print('Accuracy:     \\t{0:.4f}'.format(accuracy(y_test, y_test_pred)))\n",
    "    print('Recall:     \\t{0:.4f}'.format(recall(y_test, y_test_pred)))\n",
    "    print('Precision:     \\t{0:.4f}'.format(precision(y_test, y_test_pred)))\n",
    "    print('F1 Score:     \\t{0:.4f}'.format(f1_score(y_test, y_test_pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 - MLP (Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1030, 9)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 540.  ,    0.  ,    0.  ,  162.  ,    2.5 , 1040.  ,  676.  ,\n",
       "          28.  ,   79.99],\n",
       "       [ 540.  ,    0.  ,    0.  ,  162.  ,    2.5 , 1055.  ,  676.  ,\n",
       "          28.  ,   61.89],\n",
       "       [ 332.5 ,  142.5 ,    0.  ,  228.  ,    0.  ,  932.  ,  594.  ,\n",
       "         270.  ,   40.27]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.genfromtxt('../data/concrete.csv', delimiter=',')\n",
    "X = data[:, :-1]\n",
    "y = data[:, -1]\n",
    "print('Shape:', data.shape)\n",
    "data[:3, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows by Split\n",
      "X_train: 618 (60.0%)\n",
      "X_test:  206 (20.0%)\n",
      "X_val:   206 (20.0%)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, 0.8, random_seed=466852\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, 0.75, random_seed=654824\n",
    ")\n",
    "\n",
    "print('Number of Rows by Split')\n",
    "print('X_train: {} ({}%)'.format(X_train.shape[0], X_train.shape[0]/data.shape[0]*100))\n",
    "print('X_test:  {} ({}%)'.format(X_test.shape[0], X_test.shape[0]/data.shape[0]*100))\n",
    "print('X_val:   {} ({}%)'.format(X_val.shape[0], X_val.shape[0]/data.shape[0]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid():\n",
    "    @staticmethod\n",
    "    def get_value(x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "\n",
    "    @staticmethod\n",
    "    def get_derivative(x):\n",
    "        return Sigmoid.get_value(x) - Sigmoid.get_value(x)**2\n",
    "\n",
    "class ReLU():\n",
    "    @staticmethod\n",
    "    def get_value(x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_derivative(x):\n",
    "        if x <= 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[1., 1., 1.],\n",
       "        [1., 1., 1.]]),\n",
       " array([[1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.]])]"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyMLP():\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        self.fitted = False\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "\n",
    "        \n",
    "        self.hidden_layers_weights = []\n",
    "        self.hidden_layers_z = []\n",
    "        self.hidden_layers_activation_functions = []\n",
    "\n",
    "    \n",
    "    def get_n_neurons_of_last_layer(self):\n",
    "        if len(self.hidden_layers_weights) == 0:\n",
    "            return self.n_features\n",
    "        else:\n",
    "            return len(self.hidden_layers_weights[-1])\n",
    "\n",
    "    def add_hidden_layer(self, n_neurons, activation_function, random_state=81237):\n",
    "        seed = np.random.RandomState(random_state)\n",
    "        n_neurons_of_last_layer = self.get_n_neurons_of_last_layer()\n",
    "        print(n_neurons_of_last_layer)\n",
    "        if type(activation_function) == ReLU:\n",
    "            new_hidden_layer = seed.normal(\n",
    "                size = (n_neurons_of_last_layer+1, n_neurons)\n",
    "            ) * np.sqrt(2/n_neurons_of_last_layer)\n",
    "            new_hidden_layer[:,0] = 0.01\n",
    "            new_hidden_layer[:, :] = 1\n",
    "\n",
    "        elif type(activation_function) == Sigmoid:\n",
    "            new_hidden_layer = seed.normal(\n",
    "                size = (n_neurons, n_neurons_of_last_layer+1)\n",
    "            ) * np.sqrt(1/n_neurons_of_last_layer)\n",
    "            new_hidden_layer[:,0] = 0\n",
    "\n",
    "        self.hidden_layers_weights.append(new_hidden_layer)\n",
    "        self.hidden_layers_activation_functions.append(activation_function)\n",
    "        self.hidden_layers_z.append([])\n",
    "    \n",
    "    def set_output_layer(self, activation_function=None, n_outputs=None):\n",
    "\n",
    "        if activation_function is None:\n",
    "            self.n_outputs = 1\n",
    "        if type(activation_function) == Sigmoid:\n",
    "            self.n_outputs = 1\n",
    "        elif type(activation_function) == 'Softmax':\n",
    "            if self.n_outputs is not None:\n",
    "                self.n_outputs = n_outputs\n",
    "            else:\n",
    "                raise Exception('Parameter \\'n_outputs\\' not found.')\n",
    "        else:\n",
    "            raise Exception('Unrecognized activation function.')\n",
    "\n",
    "        self.output_activation_function = activation_function\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        X = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "        w = self.hidden_layers_weights\n",
    "\n",
    "\n",
    "        # print(X)\n",
    "        # print(w)\n",
    "        for i in range(len(w)):\n",
    "            if i == 0:\n",
    "                print(w[i])\n",
    "                print(X)\n",
    "                self.hidden_layers_z[i] = X @ w[i]\n",
    "                print(self.hidden_layers_z[i].shape)\n",
    "            else:\n",
    "                print('------------')\n",
    "                print(self.hidden_layers_z[i-1])\n",
    "                print(self.hidden_layers_z[i-1].shape[0])\n",
    "                self.hidden_layers_z[i-1] = np.hstack(\n",
    "                    [\n",
    "                        np.ones((self.hidden_layers_z[i-1].shape[0], 1)),\n",
    "                        self.hidden_layers_z[i-1]\n",
    "                    ]\n",
    "                )\n",
    "                print('NEW')\n",
    "                print(self.hidden_layers_z[i-1])\n",
    "                print(self.hidden_layers_z[i-1].shape[0])\n",
    "                print(w[i])\n",
    "                self.hidden_layers_z[i] =  self.hidden_layers_z[i-1] @ w[i]\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        pass\n",
    "\n",
    "my_mlp = MyMLP(n_features=1)\n",
    "my_mlp.add_hidden_layer(n_neurons=3, activation_function=ReLU())\n",
    "my_mlp.add_hidden_layer(n_neurons=2, activation_function=ReLU())\n",
    "# my_mlp.add_hidden_layer(n_neurons=3)\n",
    "# my_mlp.add_hidden_layer(n_neurons=3)\n",
    "my_mlp.hidden_layers_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 678,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyMLP():\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        self.fitted = False\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "\n",
    "        self.n_hidden = []\n",
    "        self.hidden_weights = []\n",
    "        self.hidden_activation_functions = [None]\n",
    "\n",
    "    def get_n_neurons_of_last_layer(self):\n",
    "        if len(self.hidden_weights) == 0:\n",
    "            return self.n_inputs\n",
    "        else:\n",
    "            return len(self.hidden_weights[-1])\n",
    "\n",
    "    def add_hidden_layer(self, n_neurons, activation_function):\n",
    "        self.n_hidden.append(n_neurons)\n",
    "        self.hidden_activation_functions = [activation_function] + self.hidden_activation_functions\n",
    "\n",
    "\n",
    "    def initialize_weights(self, random_state=8776123):\n",
    "        self.hidden_weights = []\n",
    "        \n",
    "        seed = np.random.RandomState(random_state)\n",
    "\n",
    "        layers = [self.n_inputs] + self.n_hidden + [self.n_outputs]\n",
    "        print('Layers:', layers)\n",
    "\n",
    "        for i in range(len(layers)-1):\n",
    "            # Initialization strategies\n",
    "            if type(self.hidden_activation_functions[i]) == ReLU:\n",
    "                w = seed.normal(\n",
    "                    size = (layers[i]+1, layers[i+1])\n",
    "                ) * np.sqrt(2/layers[i])  \n",
    "                w[0, :] = 0.01\n",
    "                print(w.shape)\n",
    "            else:\n",
    "                w = seed.normal(\n",
    "                    size = (layers[i]+1, layers[i+1])\n",
    "                ) * np.sqrt(1/layers[i])\n",
    "                print(w.shape)\n",
    "                w[0, :] = 0\n",
    "\n",
    "            self.hidden_weights.append(w)\n",
    "\n",
    "    #TODO - Insert BIAS terms\n",
    "    def forward_propagation(self, X):\n",
    "        activated_values = X\n",
    "        \n",
    "        for i, w in enumerate(self.hidden_weights):\n",
    "            activated_values = np.hstack([np.ones((activated_values.shape[0], 1)), activated_values])\n",
    "            activation_function = self.hidden_activation_functions[i]            \n",
    "            hidden_input = np.array(activated_values @ w)\n",
    "            print('\\n-------------------------')\n",
    "            print('Step', i)\n",
    "            print('-------------------------')\n",
    "            print('\\nInput', activated_values.shape)\n",
    "            print(activated_values)\n",
    "            print('\\nWeights', w.shape)\n",
    "            print(w)\n",
    "            print('\\nZ', hidden_input.shape)\n",
    "            print(hidden_input)\n",
    "            if activation_function is not None:\n",
    "                activated_values = activation_function.get_value(hidden_input)\n",
    "            else:\n",
    "                activated_values = hidden_input\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        return activated_values\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        pass\n",
    "\n",
    "my_mlp = MyMLP(n_inputs=2, n_outputs=1)\n",
    "my_mlp.add_hidden_layer(n_neurons=4, activation_function=ReLU())\n",
    "my_mlp.add_hidden_layer(n_neurons=2, activation_function=ReLU())\n",
    "# my_mlp.add_hidden_layer(n_neurons=2, activation_function=ReLU())\n",
    "# my_mlp.add_hidden_layer(n_neurons=3)\n",
    "# my_mlp.add_hidden_layer(n_neurons=3)\n",
    "my_mlp.hidden_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers: [2, 4, 2, 1]\n",
      "(3, 4)\n",
      "(5, 2)\n",
      "(3, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[ 0.01      ,  0.01      ,  0.01      ,  0.01      ],\n",
       "        [-0.73480594,  1.48559353, -0.52143647, -1.36089508],\n",
       "        [-0.36556347, -0.0464983 , -0.72774453, -1.54059428]]),\n",
       " array([[ 0.01      ,  0.01      ],\n",
       "        [ 0.59453462,  0.52016974],\n",
       "        [ 0.28036132, -1.22278891],\n",
       "        [-1.01657088, -1.08121814],\n",
       "        [ 0.69353916,  0.0580974 ]]),\n",
       " array([[ 0.        ],\n",
       "        [-1.63628029],\n",
       "        [-0.13923623]])]"
      ]
     },
     "execution_count": 679,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_mlp.initialize_weights()\n",
    "my_mlp.hidden_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------\n",
      "Step 0\n",
      "-------------------------\n",
      "\n",
      "Input (2, 3)\n",
      "[[1. 2. 2.]\n",
      " [1. 1. 1.]]\n",
      "\n",
      "Weights (3, 4)\n",
      "[[ 0.01        0.01        0.01        0.01      ]\n",
      " [-0.73480594  1.48559353 -0.52143647 -1.36089508]\n",
      " [-0.36556347 -0.0464983  -0.72774453 -1.54059428]]\n",
      "\n",
      "Z (2, 4)\n",
      "[[-2.19073883  2.88819046 -2.488362   -5.7929787 ]\n",
      " [-1.09036941  1.44909523 -1.239181   -2.89148935]]\n",
      "\n",
      "-------------------------\n",
      "Step 1\n",
      "-------------------------\n",
      "\n",
      "Input (2, 5)\n",
      "[[1.         0.         2.88819046 0.         0.        ]\n",
      " [1.         0.         1.44909523 0.         0.        ]]\n",
      "\n",
      "Weights (5, 2)\n",
      "[[ 0.01        0.01      ]\n",
      " [ 0.59453462  0.52016974]\n",
      " [ 0.28036132 -1.22278891]\n",
      " [-1.01657088 -1.08121814]\n",
      " [ 0.69353916  0.0580974 ]]\n",
      "\n",
      "Z (2, 2)\n",
      "[[ 0.81973688 -3.52164727]\n",
      " [ 0.41627025 -1.76193758]]\n",
      "\n",
      "-------------------------\n",
      "Step 2\n",
      "-------------------------\n",
      "\n",
      "Input (2, 3)\n",
      "[[1.         0.81973688 0.        ]\n",
      " [1.         0.41627025 0.        ]]\n",
      "\n",
      "Weights (3, 1)\n",
      "[[ 0.        ]\n",
      " [-1.63628029]\n",
      " [-0.13923623]]\n",
      "\n",
      "Z (2, 1)\n",
      "[[-1.3413193]\n",
      " [-0.6811348]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-1.3413193],\n",
       "       [-0.6811348]])"
      ]
     },
     "execution_count": 682,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[2, 2], [1, 1]])\n",
    "my_mlp.forward_propagation(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "befc19283823365b4b77b90dae63e668f3a40f1a2491259fc4e466c5bacb6daf"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('ml-project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
