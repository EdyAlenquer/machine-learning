{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment 4\n",
    "## Neural Networks\n",
    "\n",
    "Aluno: Francisco Edyvalberty Alenquer Cordeiro \\\n",
    "MatrÃ­cula: 518659\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    y_true = y_true.reshape(-1, 1)\n",
    "    y_pred = y_pred.reshape(-1, 1)\n",
    "\n",
    "    right_prediction = y_true == y_pred\n",
    "    accuracy = right_prediction.sum() / len(y_true)\n",
    "    return accuracy\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    y_true = y_true.reshape(-1, 1)\n",
    "    y_pred = y_pred.reshape(-1, 1)\n",
    "    array = np.hstack([y_true, y_pred])\n",
    "    array = array[array[:,0] == 1]\n",
    "    \n",
    "    right_prediction = array[:, 0] == array[:, 1]\n",
    "    recall = right_prediction.sum() / len(array)\n",
    "    return recall\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    y_true = y_true.reshape(-1, 1)\n",
    "    y_pred = y_pred.reshape(-1, 1)\n",
    "    array = np.hstack([y_true, y_pred])\n",
    "    array = array[array[:,1] == 1]\n",
    "    \n",
    "    right_prediction = array[:, 0] == array[:, 1]\n",
    "    precision = right_prediction.sum() / len(array)\n",
    "\n",
    "    return precision\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    y_true = y_true.reshape(-1, 1)\n",
    "    y_pred = y_pred.reshape(-1, 1)\n",
    "    precision_score = precision(y_true, y_pred)\n",
    "    recall_score = recall(y_true, y_pred)\n",
    "\n",
    "    f1_score = 2 * (precision_score * recall_score) / (precision_score + recall_score)\n",
    "\n",
    "    return f1_score\n",
    "\n",
    "def get_mse(y_real, y_pred):\n",
    "    return np.mean((y_real - y_pred)**2)\n",
    "\n",
    "def get_rmse(y_real, y_pred):\n",
    "    return np.sqrt(get_mse(y_real, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinMaxScaler:\n",
    "    def __init__(self):\n",
    "        self.fitted = False\n",
    "\n",
    "    def fit_transform(self, data):      \n",
    "        self.maximum = data.max(axis=0)\n",
    "        self.minimum = data.min(axis=0)\n",
    "        self.fitted = True\n",
    "\n",
    "        scaled_data =  (data - self.minimum) / (self.maximum - self.minimum)\n",
    "        return scaled_data\n",
    "    \n",
    "    def transform(self, data):\n",
    "        if not self.fitted:\n",
    "            raise Exception('Scaler not fitted!')\n",
    "\n",
    "        scaled_data =  (data - self.minimum) / (self.maximum - self.minimum)\n",
    "        return scaled_data\n",
    "\n",
    "    def inverse_transform(self, scaled_data):\n",
    "        if not self.fitted:\n",
    "            raise Exception('Scaler not fitted!')\n",
    "        \n",
    "        original_data = (self.maximum - self.minimum) * scaled_data + self.minimum\n",
    "        return original_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardScaler:\n",
    "    def __init__(self):\n",
    "        self.fitted = False\n",
    "\n",
    "    def fit_transform(self, data):\n",
    "        self.mean = data.mean(axis=0)\n",
    "        self.std = data.std(axis=0)\n",
    "        self.fitted = True\n",
    "\n",
    "        scaled_data = (data - self.mean) / self.std\n",
    "        return scaled_data\n",
    "\n",
    "    def transform(self, data):\n",
    "        if not self.fitted:\n",
    "            raise Exception('Scaler not fitted!')\n",
    "\n",
    "        scaled_data = (data - self.mean) / self.std\n",
    "        return scaled_data\n",
    "\n",
    "    def inverse_transform(self, scaled_data):\n",
    "        if not self.fitted:\n",
    "            raise Exception('Scaler not fitted!')\n",
    "\n",
    "        original_data = (scaled_data * self.std) + self.mean\n",
    "        return original_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cv_folds(data, n_folds=10, shuffle=True, random_state=12894):\n",
    "    indexes = np.arange(data.shape[0])\n",
    "    if shuffle:\n",
    "        np.random.seed(random_state)\n",
    "        np.random.shuffle(indexes)\n",
    "\n",
    "    slices = np.array_split(indexes, n_folds)\n",
    "    all_elements = np.hstack(slices)   \n",
    "    \n",
    "    splits = []\n",
    "    for i in range(n_folds):\n",
    "        train_idx = all_elements[~np.isin(all_elements, slices[i])]\n",
    "        test_idx = slices[i]\n",
    "\n",
    "        splits.append((train_idx, test_idx))\n",
    "\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, y, train_size_perc, random_seed=264852):\n",
    "    \n",
    "    y = y.reshape(-1, 1)\n",
    "    \n",
    "    N = X.shape[0]\n",
    "    train_size = int(train_size_perc * N)\n",
    "\n",
    "    indexes = np.arange(0, N, 1)\n",
    "\n",
    "    np.random.seed(random_seed)\n",
    "    train_idx = np.random.choice(indexes, train_size, replace=False)\n",
    "    test_idx = np.delete(indexes, train_idx)\n",
    "\n",
    "    X_train = X[train_idx, :]\n",
    "    y_train = y[train_idx, :]\n",
    "    X_test = X[test_idx, :]\n",
    "    y_test = y[test_idx, :]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do Cross Validation and Get Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_cv_and_get_metrics(classifier, cv_splits, X_train, y_train, X_test, title='Classifier', scaler=None):\n",
    "\n",
    "    X_train = X_train.copy()\n",
    "    y_train = y_train.copy()\n",
    "    X_test = X_test.copy()\n",
    "\n",
    "    train_metrics = {\n",
    "        'accuracy': [],\n",
    "        'recall': [],\n",
    "        'precision': [],\n",
    "        'f1_score': []\n",
    "    }\n",
    "\n",
    "    valid_metrics = {\n",
    "        'accuracy': [],\n",
    "        'recall': [],\n",
    "        'precision': [],\n",
    "        'f1_score': []\n",
    "    }\n",
    "    # Reporting results\n",
    "    print('#' + f'{title}'.center(60, '-') + '#')\n",
    "\n",
    "    print('\\n---> Validation Folds Metrics')\n",
    "    print('Fold\\tAccuracy\\tRecall\\t\\tPrecision\\tF1-Score')\n",
    "    count_fold = 1\n",
    "    for train_idx, val_idx in cv_splits:\n",
    "        # Spliting data\n",
    "        X_train_cv = X_train[train_idx, :]\n",
    "        y_train_cv = y_train[train_idx, :]\n",
    "        X_val_cv = X_train[val_idx, :]\n",
    "        y_val_cv = y_train[val_idx, :]\n",
    "\n",
    "        # Scaling if have scaler argument\n",
    "        if scaler is not None:\n",
    "            X_train_cv = scaler.fit_transform(X_train_cv)\n",
    "            X_val_cv = scaler.transform(X_val_cv)\n",
    "\n",
    "        # Training Model\n",
    "        classifier.fit(X_train_cv, y_train_cv.ravel())\n",
    "\n",
    "        # Predictions\n",
    "        y_train_cv_pred = classifier.predict(X_train_cv)\n",
    "        y_val_cv_pred = classifier.predict(X_val_cv)\n",
    "\n",
    "        # Storing metrics\n",
    "        train_metrics['accuracy'].append(accuracy(y_train_cv, y_train_cv_pred))\n",
    "        train_metrics['recall'].append(recall(y_train_cv, y_train_cv_pred))\n",
    "        train_metrics['precision'].append(precision(y_train_cv, y_train_cv_pred))\n",
    "        train_metrics['f1_score'].append(f1_score(y_train_cv, y_train_cv_pred))\n",
    "\n",
    "        valid_metrics['accuracy'].append(accuracy(y_val_cv, y_val_cv_pred))\n",
    "        valid_metrics['recall'].append(recall(y_val_cv, y_val_cv_pred))\n",
    "        valid_metrics['precision'].append(precision(y_val_cv, y_val_cv_pred))\n",
    "        valid_metrics['f1_score'].append(f1_score(y_val_cv, y_val_cv_pred))\n",
    "\n",
    "        print('{0:.0f}\\t{1:.4f}  \\t{2:.4f}\\t\\t{3:.4f}   \\t{4:.4f}'.format(\n",
    "                count_fold,\n",
    "                valid_metrics['accuracy'][-1], \n",
    "                valid_metrics['recall'][-1],\n",
    "                valid_metrics['precision'][-1],\n",
    "                valid_metrics['f1_score'][-1]\n",
    "            )\n",
    "        )\n",
    "        count_fold+=1\n",
    "\n",
    "\n",
    "    print('\\n--->\\tTraining Metrics')\n",
    "\n",
    "    print('Accuracy Mean:     \\t{0:.4f} | Accuracy Std:   \\t{1:.4f}'.format(\n",
    "        np.mean(train_metrics['accuracy']), \n",
    "        np.std(train_metrics['accuracy']))\n",
    "    )\n",
    "    print('Recall Mean:     \\t{0:.4f} | Recall Std:       \\t{1:.4f}'.format(\n",
    "        np.mean(train_metrics['recall']), \n",
    "        np.std(train_metrics['recall']))\n",
    "    )\n",
    "    print('Precision Mean:     \\t{0:.4f} | Precision Std:   \\t{1:.4f}'.format(\n",
    "        np.mean(train_metrics['precision']), \n",
    "        np.std(train_metrics['precision']))\n",
    "    )\n",
    "    print('F1 Score Mean:     \\t{0:.4f} | F1 Score Std:   \\t{1:.4f}'.format(\n",
    "        np.mean(train_metrics['f1_score']), \n",
    "        np.std(train_metrics['f1_score']))\n",
    "    )\n",
    "\n",
    "    print('\\n--->\\tValidation Metrics')\n",
    "\n",
    "    print('Accuracy Mean:     \\t{0:.4f} | Accuracy Std:   \\t{1:.4f}'.format(\n",
    "        np.mean(valid_metrics['accuracy']), \n",
    "        np.std(valid_metrics['accuracy']))\n",
    "    )\n",
    "    print('Recall Mean:     \\t{0:.4f} | Recall Std:       \\t{1:.4f}'.format(\n",
    "        np.mean(valid_metrics['recall']), \n",
    "        np.std(valid_metrics['recall']))\n",
    "    )\n",
    "    print('Precision Mean:     \\t{0:.4f} | Precision Std:   \\t{1:.4f}'.format(\n",
    "        np.mean(valid_metrics['precision']), \n",
    "        np.std(valid_metrics['precision']))\n",
    "    )\n",
    "    print('F1 Score Mean:     \\t{0:.4f} | F1 Score Std:   \\t{1:.4f}'.format(\n",
    "        np.mean(valid_metrics['f1_score']), \n",
    "        np.std(valid_metrics['f1_score']))\n",
    "    )\n",
    "\n",
    "    print('\\n--->\\tTest Metrics')\n",
    "\n",
    "    if scaler is not None:\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "    \n",
    "    classifier.fit(X_train, y_train.ravel())\n",
    "    y_test_pred = classifier.predict(X_test)\n",
    "\n",
    "    print('Accuracy:     \\t{0:.4f}'.format(accuracy(y_test, y_test_pred)))\n",
    "    print('Recall:     \\t{0:.4f}'.format(recall(y_test, y_test_pred)))\n",
    "    print('Precision:     \\t{0:.4f}'.format(precision(y_test, y_test_pred)))\n",
    "    print('F1 Score:     \\t{0:.4f}'.format(f1_score(y_test, y_test_pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 - MLP (Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1030, 9)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 540.  ,    0.  ,    0.  ,  162.  ,    2.5 , 1040.  ,  676.  ,\n",
       "          28.  ,   79.99],\n",
       "       [ 540.  ,    0.  ,    0.  ,  162.  ,    2.5 , 1055.  ,  676.  ,\n",
       "          28.  ,   61.89],\n",
       "       [ 332.5 ,  142.5 ,    0.  ,  228.  ,    0.  ,  932.  ,  594.  ,\n",
       "         270.  ,   40.27]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.genfromtxt('../data/concrete.csv', delimiter=',')\n",
    "X = data[:, :-1]\n",
    "y = data[:, -1]\n",
    "print('Shape:', data.shape)\n",
    "data[:3, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows by Split\n",
      "X_train: 618 (60.0%)\n",
      "X_test:  206 (20.0%)\n",
      "X_val:   206 (20.0%)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, 0.8, random_seed=466852\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, 0.75, random_seed=654824\n",
    ")\n",
    "\n",
    "print('Number of Rows by Split')\n",
    "print('X_train: {} ({}%)'.format(X_train.shape[0], X_train.shape[0]/data.shape[0]*100))\n",
    "print('X_test:  {} ({}%)'.format(X_test.shape[0], X_test.shape[0]/data.shape[0]*100))\n",
    "print('X_val:   {} ({}%)'.format(X_val.shape[0], X_val.shape[0]/data.shape[0]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaler = MinMaxScaler()\n",
    "X_train_scaled = X_scaler.fit_transform(X_train)\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y_train_scaled = y_scaler.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid():\n",
    "    @staticmethod\n",
    "    def get_value(x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "\n",
    "    @staticmethod\n",
    "    def get_derivative(x):\n",
    "        return Sigmoid.get_value(x) - Sigmoid.get_value(x)**2\n",
    "\n",
    "class ReLU():\n",
    "    @staticmethod\n",
    "    def get_value(x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_derivative(x):\n",
    "        return np.where(x <= 0, 0, 1)\n",
    "\n",
    "class Identity():\n",
    "    @staticmethod\n",
    "    def get_value(x):\n",
    "        return np.ones(x.shape) * x\n",
    "\n",
    "    @staticmethod\n",
    "    def get_derivative(x):\n",
    "        return np.ones(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Shapes\n",
    "$$\n",
    "H_i(hidden\\_weight) \\rArr (n\\_neurons(H_{i-1}), n\\_neurons(H_i))\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pesos Antes\n",
      "[array([[0. , 0. ],\n",
      "       [0.1, 0.4],\n",
      "       [0.8, 0.6]]), array([[0. ],\n",
      "       [0.3],\n",
      "       [0.9]])]\n",
      "\n",
      "Pesos DEPOIS\n",
      "[array([[0.        , 0.        ],\n",
      "       [0.09907093, 0.39713992],\n",
      "       [0.79761096, 0.59264552]]), array([[0.        ],\n",
      "       [0.27232597],\n",
      "       [0.87299836]])]\n"
     ]
    }
   ],
   "source": [
    "# FUNCIONANDO PARA 1 EXEMPLO\n",
    "class MyMLP():\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        self.fitted = False\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "\n",
    "        self.n_hidden = []\n",
    "        self.weights = []\n",
    "        self.activation_functions = [Identity(), Sigmoid()]\n",
    "        self.layer_inputs = []\n",
    "        self.layer_outputs = []\n",
    "        self.hidden_activation_derivatives = []\n",
    "        self.deltas = []\n",
    "\n",
    "    def get_n_neurons_of_last_layer(self):\n",
    "        if len(self.weights) == 0:\n",
    "            return self.n_inputs\n",
    "        else:\n",
    "            return len(self.weights[-1])\n",
    "\n",
    "    def add_hidden_layer(self, n_neurons, activation_function):\n",
    "        self.n_hidden.append(n_neurons)\n",
    "        self.activation_functions = self.activation_functions[:-1] \\\n",
    "            + [activation_function] \\\n",
    "            + [self.activation_functions[-1]]\n",
    "\n",
    "    def initialize_weights(self, random_state=8776123):\n",
    "        self.weights = []\n",
    "\n",
    "        seed = np.random.RandomState(random_state)\n",
    "        self.layers = [self.n_inputs] + self.n_hidden + [self.n_outputs]\n",
    "        \n",
    "        for i in range(len(self.layers)-1):\n",
    "            # Initialization strategies\n",
    "            if type(self.activation_functions[i]) == ReLU:\n",
    "                w = seed.normal(\n",
    "                    size = (self.layers[i]+1, self.layers[i+1])\n",
    "                ) * np.sqrt(2/self.layers[i])  \n",
    "                w[0, :] = 0.01\n",
    "            else:\n",
    "                w = seed.normal(\n",
    "                    size = (self.layers[i]+1, self.layers[i+1])\n",
    "                ) * np.sqrt(1/self.layers[i])\n",
    "                w[0, :] = 0\n",
    "\n",
    "\n",
    "            # self.weights.append(w)\n",
    "        self.weights.append(np.array([\n",
    "            [0, 0], \n",
    "            [0.1, 0.4],\n",
    "            [0.8, 0.6]\n",
    "        ]))\n",
    "        self.weights.append(np.array([\n",
    "            [0], \n",
    "            [0.3],\n",
    "            [0.9]\n",
    "        ]))\n",
    "\n",
    "\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        activated_values = X\n",
    "\n",
    "        self.layer_outputs = []\n",
    "        self.hidden_activation_derivatives = []\n",
    "\n",
    "        self.layer_outputs.append(activated_values)\n",
    "\n",
    "        for i, w in enumerate(self.weights):\n",
    "            # Insert first column with ones to multiply bias term\n",
    "            activated_values = np.hstack([np.ones((activated_values.shape[0], 1)), activated_values])\n",
    "            \n",
    "            activation_function = self.activation_functions[i+1]            \n",
    "            \n",
    "            # Calculating input of hidden layer\n",
    "            hidden_input = np.array(activated_values @ w)\n",
    "            if activation_function is not None:\n",
    "                # print('FORWARD', i)\n",
    "                # print(hidden_input)\n",
    "                # print(activation_function.get_value(hidden_input))\n",
    "                # print(activation_function)\n",
    "                activated_values = activation_function.get_value(hidden_input)\n",
    "            else:\n",
    "                activated_values = hidden_input\n",
    "                \n",
    "            self.layer_inputs.append(hidden_input)\n",
    "            self.layer_outputs.append(activated_values)\n",
    "        \n",
    "        self.layer_inputs = [X] + self.layer_inputs\n",
    "        return activated_values # Output\n",
    "\n",
    "\n",
    "    def back_propagation(self, error, alpha=1):\n",
    "\n",
    "        # calcular os deltas\n",
    "        last_layers = True\n",
    "        for i in range(len(self.activation_functions)-1, 0, -1):\n",
    "\n",
    "            act_function = self.activation_functions[i]\n",
    "            layer_inputs = self.layer_inputs[i]\n",
    "\n",
    "            if last_layers:\n",
    "                calc_i = error * act_function.get_derivative(layer_inputs)\n",
    "                self.deltas = [calc_i] + self.deltas\n",
    "                # print(self.deltas)\n",
    "                last_layers=False\n",
    "            else:\n",
    "                weights = self.weights[i][1:]\n",
    "                \n",
    "                # print('weights')\n",
    "                # print(weights.shape)\n",
    "                # print(weights)\n",
    "                # print('delta')\n",
    "                # print(self.deltas[0].shape)\n",
    "                # print(self.deltas[0])\n",
    "                # print('act_function')\n",
    "                # print(act_function.get_derivative(layer_inputs).T.shape)\n",
    "                # print(act_function.get_derivative(layer_inputs).T)\n",
    "                \n",
    "                calc_i = weights @ self.deltas[0] * act_function.get_derivative(layer_inputs).T\n",
    "                self.deltas = [calc_i] + self.deltas\n",
    "                # calc_i = error * act_function.get_derivative(layer_inputs)\n",
    "\n",
    "        # print('\\n\\n\\n----------PESOS\\n\\n\\n')\n",
    "        print('\\nPesos Antes')\n",
    "        print(self.weights)\n",
    "        for i in range(len(self.weights)):\n",
    "            # print('\\n\\n', i, '\\n\\n')\n",
    "            \n",
    "            # print(self.deltas[i])\n",
    "            # print(self.layer_outputs[i])\n",
    "            # print(self.deltas[i] @ self.layer_outputs[i])\n",
    "            # print('\\nTESTE\\n')\n",
    "            # print(self.weights[i][1:])\n",
    "            # print(gradient.T)\n",
    "            gradient = self.deltas[i] @ self.layer_outputs[i]\n",
    "            self.weights[i][1:, :] = self.weights[i][1:, :] - alpha * gradient.T\n",
    "\n",
    "            # print(i)  \n",
    "            # print(self.layer_inputs[i])\n",
    "        print('\\nPesos DEPOIS')\n",
    "        print(self.weights)\n",
    "\n",
    "\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        pass\n",
    "\n",
    "my_mlp = MyMLP(n_inputs=2, n_outputs=2)\n",
    "my_mlp.add_hidden_layer(n_neurons=2, activation_function=Sigmoid())\n",
    "# my_mlp.add_hidden_layer(n_neurons=2, activation_function=ReLU())\n",
    "\n",
    "my_mlp.initialize_weights()\n",
    "\n",
    "X = np.array([[0.35, 0.9]])\n",
    "y = np.array([[0.5]])\n",
    "\n",
    "y_pred = my_mlp.forward_propagation(X)\n",
    "error = y_pred - y\n",
    "\n",
    "my_mlp.back_propagation(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Epoch 0: 0.04805965977105774\n",
      "MSE Epoch 1: 0.039076521861883734\n",
      "MSE Epoch 2: 0.03447773109851601\n",
      "MSE Epoch 3: 0.03113414148230009\n",
      "MSE Epoch 4: 0.02796634479332156\n",
      "MSE Epoch 5: 0.024472091823557005\n",
      "MSE Epoch 6: 0.02181923695054833\n",
      "MSE Epoch 7: 0.020516002779436278\n",
      "MSE Epoch 8: 0.019837233405229016\n",
      "MSE Epoch 9: 0.0193823749327655\n",
      "MSE Epoch 10: 0.01905162777127944\n",
      "MSE Epoch 11: 0.018775507509445587\n",
      "MSE Epoch 12: 0.01851138010872693\n",
      "MSE Epoch 13: 0.018198149985739414\n",
      "MSE Epoch 14: 0.01790077179872681\n",
      "MSE Epoch 15: 0.01760338274502916\n",
      "MSE Epoch 16: 0.017350367823752048\n",
      "MSE Epoch 17: 0.01709170961266884\n",
      "MSE Epoch 18: 0.016864421319616518\n",
      "MSE Epoch 19: 0.01663760389634482\n",
      "MSE Epoch 20: 0.016411802724190756\n",
      "MSE Epoch 21: 0.016182675908530383\n",
      "MSE Epoch 22: 0.015983542871350797\n",
      "MSE Epoch 23: 0.015815470169183582\n",
      "MSE Epoch 24: 0.015658281410907485\n",
      "MSE Epoch 25: 0.015518215638466162\n",
      "MSE Epoch 26: 0.015383341481868318\n",
      "MSE Epoch 27: 0.015263298986924632\n",
      "MSE Epoch 28: 0.015136112933112398\n",
      "MSE Epoch 29: 0.015030767409098226\n",
      "MSE Epoch 30: 0.014918196225791583\n",
      "MSE Epoch 31: 0.014791446490176356\n",
      "MSE Epoch 32: 0.014652740700711023\n",
      "MSE Epoch 33: 0.014511542418944031\n",
      "MSE Epoch 34: 0.014376859887100746\n",
      "MSE Epoch 35: 0.01424642712230474\n",
      "MSE Epoch 36: 0.014121632546898315\n",
      "MSE Epoch 37: 0.013998848663689554\n",
      "MSE Epoch 38: 0.013864523724161186\n",
      "MSE Epoch 39: 0.013729046397121813\n",
      "MSE Epoch 40: 0.013600258914809166\n",
      "MSE Epoch 41: 0.013460455731968822\n",
      "MSE Epoch 42: 0.013345856504173815\n",
      "MSE Epoch 43: 0.013227125431172352\n",
      "MSE Epoch 44: 0.013115343012235483\n",
      "MSE Epoch 45: 0.013008198912060563\n",
      "MSE Epoch 46: 0.012900964718823817\n",
      "MSE Epoch 47: 0.012791817962242729\n",
      "MSE Epoch 48: 0.012699850686787079\n",
      "MSE Epoch 49: 0.012577771089798583\n",
      "MSE Epoch 50: 0.01239586305077494\n",
      "MSE Epoch 51: 0.012188789206523448\n",
      "MSE Epoch 52: 0.012000900843497583\n",
      "MSE Epoch 53: 0.011830665539720089\n",
      "MSE Epoch 54: 0.011667106438506365\n",
      "MSE Epoch 55: 0.011520177709814106\n",
      "MSE Epoch 56: 0.011367255985218436\n",
      "MSE Epoch 57: 0.011197762500473147\n",
      "MSE Epoch 58: 0.01106341106737853\n",
      "MSE Epoch 59: 0.01093445994932598\n",
      "MSE Epoch 60: 0.010823521644633325\n",
      "MSE Epoch 61: 0.010724643583602587\n",
      "MSE Epoch 62: 0.010627733898186035\n",
      "MSE Epoch 63: 0.010543818083255118\n",
      "MSE Epoch 64: 0.01045604691112387\n",
      "MSE Epoch 65: 0.010374848891574721\n",
      "MSE Epoch 66: 0.010298308821166804\n",
      "MSE Epoch 67: 0.010228727463899044\n",
      "MSE Epoch 68: 0.010157851353945367\n",
      "MSE Epoch 69: 0.01008241667946385\n",
      "MSE Epoch 70: 0.010025887812843377\n",
      "MSE Epoch 71: 0.009963360183036736\n",
      "MSE Epoch 72: 0.009898321993145102\n",
      "MSE Epoch 73: 0.009846601908169002\n",
      "MSE Epoch 74: 0.009790426756972954\n",
      "MSE Epoch 75: 0.009726974480260232\n",
      "MSE Epoch 76: 0.009657425623151427\n",
      "MSE Epoch 77: 0.009598788310269559\n",
      "MSE Epoch 78: 0.009549395379253846\n",
      "MSE Epoch 79: 0.009482976234162435\n",
      "MSE Epoch 80: 0.009427099100401288\n",
      "MSE Epoch 81: 0.009355702635114139\n",
      "MSE Epoch 82: 0.009294321489222553\n",
      "MSE Epoch 83: 0.00924509875823044\n",
      "MSE Epoch 84: 0.009203549796849878\n",
      "MSE Epoch 85: 0.009149887826975308\n",
      "MSE Epoch 86: 0.009114958289618426\n",
      "MSE Epoch 87: 0.009080378712755001\n",
      "MSE Epoch 88: 0.009045558541565307\n",
      "MSE Epoch 89: 0.009019102939983977\n",
      "MSE Epoch 90: 0.008986160825397884\n",
      "MSE Epoch 91: 0.008967317112953937\n",
      "MSE Epoch 92: 0.008933525872563521\n",
      "MSE Epoch 93: 0.008917740919184437\n",
      "MSE Epoch 94: 0.008894778898406959\n",
      "MSE Epoch 95: 0.008867058433518081\n",
      "MSE Epoch 96: 0.00885524758566433\n",
      "MSE Epoch 97: 0.008833791521026629\n",
      "MSE Epoch 98: 0.00881666874280068\n",
      "MSE Epoch 99: 0.008797725887880364\n",
      "MSE Epoch 100: 0.008780944145090461\n",
      "MSE Epoch 101: 0.008764134117319442\n",
      "MSE Epoch 102: 0.008747526858503472\n",
      "MSE Epoch 103: 0.008736856007331147\n",
      "MSE Epoch 104: 0.008717919378961247\n",
      "MSE Epoch 105: 0.008704424751352144\n",
      "MSE Epoch 106: 0.008688125442172833\n",
      "MSE Epoch 107: 0.008676248528604802\n",
      "MSE Epoch 108: 0.0086619683496133\n",
      "MSE Epoch 109: 0.00865194906014195\n",
      "MSE Epoch 110: 0.00864112459735708\n",
      "MSE Epoch 111: 0.008629355562384521\n",
      "MSE Epoch 112: 0.008618277717509913\n",
      "MSE Epoch 113: 0.008601959400607428\n",
      "MSE Epoch 114: 0.008597030504819263\n",
      "MSE Epoch 115: 0.008578670199319712\n",
      "MSE Epoch 116: 0.008577429996821777\n",
      "MSE Epoch 117: 0.008564493528216552\n",
      "MSE Epoch 118: 0.008543000978922851\n",
      "MSE Epoch 119: 0.008541490823466577\n",
      "MSE Epoch 120: 0.008534012003436755\n",
      "MSE Epoch 121: 0.008520895955023795\n",
      "MSE Epoch 122: 0.008516446080282649\n",
      "MSE Epoch 123: 0.008501381883416018\n",
      "MSE Epoch 124: 0.008496238148111268\n",
      "MSE Epoch 125: 0.008484131237436925\n",
      "MSE Epoch 126: 0.00848054363140281\n",
      "MSE Epoch 127: 0.008466308833304886\n",
      "MSE Epoch 128: 0.008464334235700375\n",
      "MSE Epoch 129: 0.008449678142476894\n",
      "MSE Epoch 130: 0.008447555055625411\n",
      "MSE Epoch 131: 0.008435624571731568\n",
      "MSE Epoch 132: 0.00843028935063441\n",
      "MSE Epoch 133: 0.008420686885607757\n",
      "MSE Epoch 134: 0.008404025580198654\n",
      "MSE Epoch 135: 0.008404905860074668\n",
      "MSE Epoch 136: 0.008403989631239037\n",
      "MSE Epoch 137: 0.008396861365308518\n",
      "MSE Epoch 138: 0.008391698211385763\n",
      "MSE Epoch 139: 0.008372300099445824\n",
      "MSE Epoch 140: 0.008369202351230221\n",
      "MSE Epoch 141: 0.008364986651591702\n",
      "MSE Epoch 142: 0.008358296655352045\n",
      "MSE Epoch 143: 0.008350006648392206\n",
      "MSE Epoch 144: 0.00834679436455967\n",
      "MSE Epoch 145: 0.008345653541762565\n",
      "MSE Epoch 146: 0.008336964014044372\n",
      "MSE Epoch 147: 0.00832923379032846\n",
      "MSE Epoch 148: 0.008323933721366085\n",
      "MSE Epoch 149: 0.008318288056757968\n",
      "MSE Epoch 150: 0.008310918844244937\n",
      "MSE Epoch 151: 0.008306745198569988\n",
      "MSE Epoch 152: 0.0082998507939359\n",
      "MSE Epoch 153: 0.008301673974620843\n",
      "MSE Epoch 154: 0.008294774143685147\n",
      "MSE Epoch 155: 0.008300846404190614\n",
      "MSE Epoch 156: 0.00829013593355105\n",
      "MSE Epoch 157: 0.0082913441363703\n",
      "MSE Epoch 158: 0.008281070142363406\n",
      "MSE Epoch 159: 0.0082750871100341\n",
      "MSE Epoch 160: 0.008273510482237819\n",
      "MSE Epoch 161: 0.008264768042326426\n",
      "MSE Epoch 162: 0.008268208977797249\n",
      "MSE Epoch 163: 0.008257539908525322\n",
      "MSE Epoch 164: 0.008260920374938708\n",
      "MSE Epoch 165: 0.008261091594318334\n",
      "MSE Epoch 166: 0.008251369657101775\n",
      "MSE Epoch 167: 0.008251102638373623\n",
      "MSE Epoch 168: 0.00824226703651335\n",
      "MSE Epoch 169: 0.008245641283054793\n",
      "MSE Epoch 170: 0.00824467865218257\n",
      "MSE Epoch 171: 0.008231853977773915\n",
      "MSE Epoch 172: 0.008241800667370921\n",
      "MSE Epoch 173: 0.008225895177345732\n",
      "MSE Epoch 174: 0.008238108709946866\n",
      "MSE Epoch 175: 0.00822897944716584\n",
      "MSE Epoch 176: 0.008229844164184692\n",
      "MSE Epoch 177: 0.008228107855150369\n",
      "MSE Epoch 178: 0.008215755789978418\n",
      "MSE Epoch 179: 0.008225578256138922\n",
      "MSE Epoch 180: 0.008215674088751875\n",
      "MSE Epoch 181: 0.008218920382575379\n",
      "MSE Epoch 182: 0.008218323612089067\n",
      "MSE Epoch 183: 0.008213749294123015\n",
      "MSE Epoch 184: 0.008214651605825483\n",
      "MSE Epoch 185: 0.008202558580154165\n",
      "MSE Epoch 186: 0.00821050721758259\n",
      "MSE Epoch 187: 0.008210123551528292\n",
      "MSE Epoch 188: 0.0082053107749222\n",
      "MSE Epoch 189: 0.008205717413528268\n",
      "MSE Epoch 190: 0.008194473690010069\n",
      "MSE Epoch 191: 0.008204625588248025\n",
      "MSE Epoch 192: 0.008205440027242088\n",
      "MSE Epoch 193: 0.00819291310901241\n",
      "MSE Epoch 194: 0.008199852986885204\n",
      "MSE Epoch 195: 0.008199124352580306\n",
      "MSE Epoch 196: 0.008192574822612151\n",
      "MSE Epoch 197: 0.008196004991243622\n",
      "MSE Epoch 198: 0.008194894790659492\n",
      "MSE Epoch 199: 0.008195611434927348\n",
      "MSE Epoch 200: 0.00818066683487622\n",
      "MSE Epoch 201: 0.008181667269985829\n",
      "MSE Epoch 202: 0.008175358949897545\n",
      "MSE Epoch 203: 0.00818139487051153\n",
      "MSE Epoch 204: 0.00818031543313508\n",
      "MSE Epoch 205: 0.008174476146194955\n",
      "MSE Epoch 206: 0.008177191234760003\n",
      "MSE Epoch 207: 0.00817152115625921\n",
      "MSE Epoch 208: 0.008168091794973978\n",
      "MSE Epoch 209: 0.008165696567466997\n",
      "MSE Epoch 210: 0.008164608362202383\n",
      "MSE Epoch 211: 0.008170500841068677\n",
      "MSE Epoch 212: 0.008166121790728148\n",
      "MSE Epoch 213: 0.008168514290130739\n",
      "MSE Epoch 214: 0.008167527387623852\n",
      "MSE Epoch 215: 0.008162782349882483\n",
      "MSE Epoch 216: 0.008159273507196783\n",
      "MSE Epoch 217: 0.008157061515162092\n",
      "MSE Epoch 218: 0.008163456599387492\n",
      "MSE Epoch 219: 0.008184244189674674\n",
      "MSE Epoch 220: 0.008183044106797484\n",
      "MSE Epoch 221: 0.00817429603192817\n",
      "MSE Epoch 222: 0.008170324364671494\n",
      "MSE Epoch 223: 0.008167256805010738\n",
      "MSE Epoch 224: 0.008162816635053147\n",
      "MSE Epoch 225: 0.008161243223782078\n",
      "MSE Epoch 226: 0.00816685494889588\n",
      "MSE Epoch 227: 0.008165106576137306\n",
      "MSE Epoch 228: 0.008161238839690794\n",
      "MSE Epoch 229: 0.008158139892410522\n",
      "MSE Epoch 230: 0.008156588198849095\n",
      "MSE Epoch 231: 0.008162448278806507\n",
      "MSE Epoch 232: 0.008160862517996477\n",
      "MSE Epoch 233: 0.008154889004799697\n",
      "MSE Epoch 234: 0.008159036630713204\n",
      "MSE Epoch 235: 0.00815795131768612\n",
      "MSE Epoch 236: 0.008152689014432198\n",
      "MSE Epoch 237: 0.008153989276393004\n",
      "MSE Epoch 238: 0.008151389868391024\n",
      "MSE Epoch 239: 0.00814953841452094\n",
      "MSE Epoch 240: 0.008155171069788135\n",
      "MSE Epoch 241: 0.008148055619437981\n",
      "MSE Epoch 242: 0.008152986441563996\n",
      "MSE Epoch 243: 0.00814729289549783\n",
      "MSE Epoch 244: 0.008146777274303654\n",
      "MSE Epoch 245: 0.00814623850232862\n",
      "MSE Epoch 246: 0.008150158899371531\n",
      "MSE Epoch 247: 0.008151916048579868\n",
      "MSE Epoch 248: 0.008146033597817065\n",
      "MSE Epoch 249: 0.00814231404346934\n",
      "MSE Epoch 250: 0.008141150606085189\n",
      "MSE Epoch 251: 0.008148025628148361\n",
      "MSE Epoch 252: 0.00814266657459888\n",
      "MSE Epoch 253: 0.008140564041485587\n",
      "MSE Epoch 254: 0.008146495192217584\n",
      "MSE Epoch 255: 0.008138780213064306\n",
      "MSE Epoch 256: 0.00814552601557582\n",
      "MSE Epoch 257: 0.008140582231516116\n",
      "MSE Epoch 258: 0.008136978892613686\n",
      "MSE Epoch 259: 0.008138948386120599\n",
      "MSE Epoch 260: 0.008143349143361218\n",
      "MSE Epoch 261: 0.008138667833366538\n",
      "MSE Epoch 262: 0.008135352985332604\n",
      "MSE Epoch 263: 0.00814306347060704\n",
      "MSE Epoch 264: 0.00813739943333457\n",
      "MSE Epoch 265: 0.008134057408608415\n",
      "MSE Epoch 266: 0.008136202929398342\n",
      "MSE Epoch 267: 0.008140716514083468\n",
      "MSE Epoch 268: 0.008136011251245275\n",
      "MSE Epoch 269: 0.008132621606655713\n",
      "MSE Epoch 270: 0.008140525478638336\n",
      "MSE Epoch 271: 0.008134852936622566\n",
      "MSE Epoch 272: 0.008131454086911759\n",
      "MSE Epoch 273: 0.0081333187989245\n",
      "MSE Epoch 274: 0.008138317284286243\n",
      "MSE Epoch 275: 0.008133625278798347\n",
      "MSE Epoch 276: 0.008130150932674435\n",
      "MSE Epoch 277: 0.008138165642347256\n",
      "MSE Epoch 278: 0.008132543190334853\n",
      "MSE Epoch 279: 0.008129098040210394\n",
      "MSE Epoch 280: 0.008131063846579623\n",
      "MSE Epoch 281: 0.00813617550924117\n",
      "MSE Epoch 282: 0.008131419481660898\n",
      "MSE Epoch 283: 0.008127912274521903\n",
      "MSE Epoch 284: 0.008135891318853398\n",
      "MSE Epoch 285: 0.008130483730661076\n",
      "MSE Epoch 286: 0.008126901305208743\n",
      "MSE Epoch 287: 0.008128951921894135\n",
      "MSE Epoch 288: 0.008133882754608735\n",
      "MSE Epoch 289: 0.008129477172620013\n",
      "MSE Epoch 290: 0.008125855665498755\n",
      "MSE Epoch 291: 0.00812775676233602\n",
      "MSE Epoch 292: 0.008132595670394918\n",
      "MSE Epoch 293: 0.008128321623693436\n",
      "MSE Epoch 294: 0.008124775131309436\n",
      "MSE Epoch 295: 0.008126610355961905\n",
      "MSE Epoch 296: 0.008131429393367506\n",
      "MSE Epoch 297: 0.008126760263227409\n",
      "MSE Epoch 298: 0.008127008396900707\n",
      "MSE Epoch 299: 0.008130448961485176\n",
      "MSE Epoch 300: 0.008123197862019291\n",
      "MSE Epoch 301: 0.008125068225983124\n",
      "MSE Epoch 302: 0.008125020328629318\n",
      "MSE Epoch 303: 0.008129452296590347\n",
      "MSE Epoch 304: 0.008122066015848856\n",
      "MSE Epoch 305: 0.00812320362990959\n",
      "MSE Epoch 306: 0.008128131312673752\n",
      "MSE Epoch 307: 0.008121198783181688\n",
      "MSE Epoch 308: 0.008123145755560524\n",
      "MSE Epoch 309: 0.008123113577782233\n",
      "MSE Epoch 310: 0.008127477282574338\n",
      "MSE Epoch 311: 0.008120263774582413\n",
      "MSE Epoch 312: 0.008121389132393532\n",
      "MSE Epoch 313: 0.00812625200383888\n",
      "MSE Epoch 314: 0.008122264957252897\n",
      "MSE Epoch 315: 0.008119309212490853\n",
      "MSE Epoch 316: 0.00812101911242111\n",
      "MSE Epoch 317: 0.0081253759560934\n",
      "MSE Epoch 318: 0.008121161506558667\n",
      "MSE Epoch 319: 0.008118083769031007\n",
      "MSE Epoch 320: 0.008119957651408242\n",
      "MSE Epoch 321: 0.008124481906959826\n",
      "MSE Epoch 322: 0.008120107941429481\n",
      "MSE Epoch 323: 0.00811714743747685\n",
      "MSE Epoch 324: 0.008118304104667896\n",
      "MSE Epoch 325: 0.008123673206389143\n",
      "MSE Epoch 326: 0.008119192744882106\n",
      "MSE Epoch 327: 0.008122697479317846\n",
      "MSE Epoch 328: 0.008117202072344686\n",
      "MSE Epoch 329: 0.008117791634847157\n",
      "MSE Epoch 330: 0.008117780735439938\n",
      "MSE Epoch 331: 0.008122036896913562\n",
      "MSE Epoch 332: 0.008115384085609542\n",
      "MSE Epoch 333: 0.008116140409144502\n",
      "MSE Epoch 334: 0.008120942808677178\n",
      "MSE Epoch 335: 0.008114594021877548\n",
      "MSE Epoch 336: 0.008115510903137868\n",
      "MSE Epoch 337: 0.008120183976093983\n",
      "MSE Epoch 338: 0.008113828605858362\n",
      "MSE Epoch 339: 0.008115618835827058\n",
      "MSE Epoch 340: 0.008115535970336029\n",
      "MSE Epoch 341: 0.00811989110170928\n",
      "MSE Epoch 342: 0.008113116809774626\n",
      "MSE Epoch 343: 0.00811394571986126\n",
      "MSE Epoch 344: 0.008118770897804566\n",
      "MSE Epoch 345: 0.00811493091742382\n",
      "MSE Epoch 346: 0.008112153965550501\n",
      "MSE Epoch 347: 0.00811336979847439\n",
      "MSE Epoch 348: 0.008118172174593263\n",
      "MSE Epoch 349: 0.008113995539598367\n",
      "MSE Epoch 350: 0.008111163628243316\n",
      "MSE Epoch 351: 0.008112140925382061\n",
      "MSE Epoch 352: 0.008117475098371482\n",
      "MSE Epoch 353: 0.008110684143036962\n",
      "MSE Epoch 354: 0.008111589382674983\n",
      "MSE Epoch 355: 0.008116556017239039\n",
      "MSE Epoch 356: 0.008109946084261377\n",
      "MSE Epoch 357: 0.008111722480836981\n",
      "MSE Epoch 358: 0.008115802368874577\n",
      "MSE Epoch 359: 0.008111900683699724\n",
      "MSE Epoch 360: 0.008108955557345914\n",
      "MSE Epoch 361: 0.008110378872792269\n",
      "MSE Epoch 362: 0.008115207407890743\n",
      "MSE Epoch 363: 0.008111042668857803\n",
      "MSE Epoch 364: 0.008114426314604382\n",
      "MSE Epoch 365: 0.008109252640413648\n",
      "MSE Epoch 366: 0.008108801151314175\n",
      "MSE Epoch 367: 0.008107309401743718\n",
      "MSE Epoch 368: 0.008108673368412702\n",
      "MSE Epoch 369: 0.008113963884922621\n",
      "MSE Epoch 370: 0.008109703295562284\n",
      "MSE Epoch 371: 0.008113138447881673\n",
      "MSE Epoch 372: 0.00810994583147397\n",
      "MSE Epoch 373: 0.008108600525873725\n",
      "MSE Epoch 374: 0.008112929688801543\n",
      "MSE Epoch 375: 0.008107386052023767\n",
      "MSE Epoch 376: 0.008106761423227337\n",
      "MSE Epoch 377: 0.00811140482831487\n",
      "MSE Epoch 378: 0.008106648798154549\n",
      "MSE Epoch 379: 0.008106474453676342\n",
      "MSE Epoch 380: 0.008110875699204776\n",
      "MSE Epoch 381: 0.008105039834361373\n",
      "MSE Epoch 382: 0.008105856934434833\n",
      "MSE Epoch 383: 0.008110199572072251\n",
      "MSE Epoch 384: 0.008104516380367943\n",
      "MSE Epoch 385: 0.008105270664311328\n",
      "MSE Epoch 386: 0.008109687544061843\n",
      "MSE Epoch 387: 0.008103990681895039\n",
      "MSE Epoch 388: 0.008104751689597194\n",
      "MSE Epoch 389: 0.008105209615302493\n",
      "MSE Epoch 390: 0.008109520889959445\n",
      "MSE Epoch 391: 0.008103460788611001\n",
      "MSE Epoch 392: 0.008104003716423515\n",
      "MSE Epoch 393: 0.008108577817036285\n",
      "MSE Epoch 394: 0.008102744834140528\n",
      "MSE Epoch 395: 0.008103463603303766\n",
      "MSE Epoch 396: 0.008107928635710646\n",
      "MSE Epoch 397: 0.008101947016821827\n",
      "MSE Epoch 398: 0.008102818324632912\n",
      "MSE Epoch 399: 0.00810719949097297\n",
      "MSE Epoch 400: 0.008103778433557347\n",
      "MSE Epoch 401: 0.00810124741469346\n",
      "MSE Epoch 402: 0.008101746935972584\n",
      "MSE Epoch 403: 0.008106794508527531\n",
      "MSE Epoch 404: 0.008100827072144066\n",
      "MSE Epoch 405: 0.008101515844862043\n",
      "MSE Epoch 406: 0.008106081644279766\n",
      "MSE Epoch 407: 0.008100259059416077\n",
      "MSE Epoch 408: 0.008100978525627559\n",
      "MSE Epoch 409: 0.008105451366036224\n",
      "MSE Epoch 410: 0.008099479380525047\n",
      "MSE Epoch 411: 0.00810035889056967\n",
      "MSE Epoch 412: 0.008104728989335088\n",
      "MSE Epoch 413: 0.008101053576992548\n",
      "MSE Epoch 414: 0.008098837945674335\n",
      "MSE Epoch 415: 0.008099287360667522\n",
      "MSE Epoch 416: 0.008104304627849794\n",
      "MSE Epoch 417: 0.008098433477258428\n",
      "MSE Epoch 418: 0.008099074589521455\n",
      "MSE Epoch 419: 0.008103615110109407\n",
      "MSE Epoch 420: 0.00809789108307518\n",
      "MSE Epoch 421: 0.008096481278906461\n",
      "MSE Epoch 422: 0.008103708817879415\n",
      "MSE Epoch 423: 0.008095294185322429\n",
      "MSE Epoch 424: 0.00809450048796027\n",
      "MSE Epoch 425: 0.008101330260505227\n",
      "MSE Epoch 426: 0.008093191980563769\n",
      "MSE Epoch 427: 0.008094222724574872\n",
      "MSE Epoch 428: 0.008099610725890053\n",
      "MSE Epoch 429: 0.008098949122339858\n",
      "MSE Epoch 430: 0.008093324064699549\n",
      "MSE Epoch 431: 0.00809756516749105\n",
      "MSE Epoch 432: 0.008094832677925174\n",
      "MSE Epoch 433: 0.008097522191625321\n",
      "MSE Epoch 434: 0.008094585655935538\n",
      "MSE Epoch 435: 0.00809784242638355\n",
      "MSE Epoch 436: 0.008093918950882965\n",
      "MSE Epoch 437: 0.008092835969971084\n",
      "MSE Epoch 438: 0.008098269559306355\n",
      "MSE Epoch 439: 0.008095267385675172\n",
      "MSE Epoch 440: 0.008090093914796585\n",
      "MSE Epoch 441: 0.008091281408551048\n",
      "MSE Epoch 442: 0.00809656967473803\n",
      "MSE Epoch 443: 0.008093009338799613\n",
      "MSE Epoch 444: 0.008096615509635972\n",
      "MSE Epoch 445: 0.008094073663299256\n",
      "MSE Epoch 446: 0.00808937803228182\n",
      "MSE Epoch 447: 0.008090207807622866\n",
      "MSE Epoch 448: 0.008096059278178499\n",
      "MSE Epoch 449: 0.00809355107965622\n",
      "MSE Epoch 450: 0.008090526331920806\n",
      "MSE Epoch 451: 0.008092338586259165\n",
      "MSE Epoch 452: 0.008088081972953345\n",
      "MSE Epoch 453: 0.00808923165373947\n",
      "MSE Epoch 454: 0.00809502359471383\n",
      "MSE Epoch 455: 0.008092315730988798\n",
      "MSE Epoch 456: 0.008089553651625668\n",
      "MSE Epoch 457: 0.008093811287171914\n",
      "MSE Epoch 458: 0.008090647189347742\n",
      "MSE Epoch 459: 0.008091741530247339\n",
      "MSE Epoch 460: 0.00808877614425286\n",
      "MSE Epoch 461: 0.008090970793064341\n",
      "MSE Epoch 462: 0.008086395553496096\n",
      "MSE Epoch 463: 0.008086649515302682\n",
      "MSE Epoch 464: 0.008094268530872384\n",
      "MSE Epoch 465: 0.008090598051001657\n",
      "MSE Epoch 466: 0.008085805058189352\n",
      "MSE Epoch 467: 0.008086981472245345\n",
      "MSE Epoch 468: 0.008089869940519943\n",
      "MSE Epoch 469: 0.008085912511403533\n",
      "MSE Epoch 470: 0.008093130265596538\n",
      "MSE Epoch 471: 0.008089517069881097\n",
      "MSE Epoch 472: 0.008084721820248632\n",
      "MSE Epoch 473: 0.008085892488741843\n",
      "MSE Epoch 474: 0.008088637333263463\n",
      "MSE Epoch 475: 0.008085681008716398\n",
      "MSE Epoch 476: 0.008090932290593761\n",
      "MSE Epoch 477: 0.008088526691679011\n",
      "MSE Epoch 478: 0.008085675549477466\n",
      "MSE Epoch 479: 0.008086407424450777\n",
      "MSE Epoch 480: 0.008090056673254166\n",
      "MSE Epoch 481: 0.008086658993864808\n",
      "MSE Epoch 482: 0.00808796932000602\n",
      "MSE Epoch 483: 0.008082674807022983\n",
      "MSE Epoch 484: 0.008088897045112262\n",
      "MSE Epoch 485: 0.008083440758491649\n",
      "MSE Epoch 486: 0.00808837857939836\n",
      "MSE Epoch 487: 0.008083189224555445\n",
      "MSE Epoch 488: 0.008088098228435283\n",
      "MSE Epoch 489: 0.008082881213047968\n",
      "MSE Epoch 490: 0.0080877810119485\n",
      "MSE Epoch 491: 0.008082559434042175\n",
      "MSE Epoch 492: 0.008087838222247732\n",
      "MSE Epoch 493: 0.008087733808852117\n",
      "MSE Epoch 494: 0.008084741857411367\n",
      "MSE Epoch 495: 0.008085487601577308\n",
      "MSE Epoch 496: 0.008080755094032164\n",
      "MSE Epoch 497: 0.008081892667387583\n",
      "MSE Epoch 498: 0.008086645254669613\n",
      "MSE Epoch 499: 0.008083477327589841\n",
      "MSE Epoch 500: 0.008084717065160044\n",
      "MSE Epoch 501: 0.008079526662144296\n",
      "MSE Epoch 502: 0.008085630355788099\n",
      "MSE Epoch 503: 0.008085988159782289\n",
      "MSE Epoch 504: 0.008082982268595683\n",
      "MSE Epoch 505: 0.008083474967123774\n",
      "MSE Epoch 506: 0.008079088488037433\n",
      "MSE Epoch 507: 0.008080238692620182\n",
      "MSE Epoch 508: 0.008085734680655564\n",
      "MSE Epoch 509: 0.008083402706342372\n",
      "MSE Epoch 510: 0.008078564422254152\n",
      "MSE Epoch 511: 0.008079336815940988\n",
      "MSE Epoch 512: 0.008085098482638846\n",
      "MSE Epoch 513: 0.008082491654686344\n",
      "MSE Epoch 514: 0.008077878848854466\n",
      "MSE Epoch 515: 0.008078730061770381\n",
      "MSE Epoch 516: 0.008084436235057734\n",
      "MSE Epoch 517: 0.00808206565109046\n",
      "MSE Epoch 518: 0.008077352221673915\n",
      "MSE Epoch 519: 0.008078275290476132\n",
      "MSE Epoch 520: 0.008081320176060999\n",
      "MSE Epoch 521: 0.008077660546796633\n",
      "MSE Epoch 522: 0.008084137599917402\n",
      "MSE Epoch 523: 0.008081037161409748\n",
      "MSE Epoch 524: 0.008076318386428692\n",
      "MSE Epoch 525: 0.00807725578274267\n",
      "MSE Epoch 526: 0.008082677797526123\n",
      "MSE Epoch 527: 0.008080311063674978\n",
      "MSE Epoch 528: 0.008075844669801274\n",
      "MSE Epoch 529: 0.00807665981799386\n",
      "MSE Epoch 530: 0.008082066433123761\n",
      "MSE Epoch 531: 0.008079879356128871\n",
      "MSE Epoch 532: 0.008076752741127135\n",
      "MSE Epoch 533: 0.00807915086680767\n",
      "MSE Epoch 534: 0.008074845991940275\n",
      "MSE Epoch 535: 0.008073859351616273\n",
      "MSE Epoch 536: 0.008080830179645627\n",
      "MSE Epoch 537: 0.008080727657390364\n",
      "MSE Epoch 538: 0.008075870761067448\n",
      "MSE Epoch 539: 0.008075175824348593\n",
      "MSE Epoch 540: 0.008078410267380154\n",
      "MSE Epoch 541: 0.008073334194912882\n",
      "MSE Epoch 542: 0.008079424694468512\n",
      "MSE Epoch 543: 0.008079728312594427\n",
      "MSE Epoch 544: 0.0080748278538568\n",
      "MSE Epoch 545: 0.008073936118949866\n",
      "MSE Epoch 546: 0.008077718052371704\n",
      "MSE Epoch 547: 0.008072537761739594\n",
      "MSE Epoch 548: 0.008078339589203457\n",
      "MSE Epoch 549: 0.008078688284092174\n",
      "MSE Epoch 550: 0.008073861443736388\n",
      "MSE Epoch 551: 0.008077469049000436\n",
      "MSE Epoch 552: 0.008074724423149394\n",
      "MSE Epoch 553: 0.008076258881691214\n",
      "MSE Epoch 554: 0.008071525498928717\n",
      "MSE Epoch 555: 0.008077392419527022\n",
      "MSE Epoch 556: 0.008073210041385358\n",
      "MSE Epoch 557: 0.008077858710475113\n",
      "MSE Epoch 558: 0.00807421350246842\n",
      "MSE Epoch 559: 0.008075352040438374\n",
      "MSE Epoch 560: 0.008070595040115952\n",
      "MSE Epoch 561: 0.008076497873130513\n",
      "MSE Epoch 562: 0.008072201493432483\n",
      "MSE Epoch 563: 0.008076597841574185\n",
      "MSE Epoch 564: 0.008072181588287129\n",
      "MSE Epoch 565: 0.008074449596886022\n",
      "MSE Epoch 566: 0.008069439200094926\n",
      "MSE Epoch 567: 0.008075196174933312\n",
      "MSE Epoch 568: 0.008075557079427323\n",
      "MSE Epoch 569: 0.00807274904675934\n",
      "MSE Epoch 570: 0.008073505720893253\n",
      "MSE Epoch 571: 0.008069227016700728\n",
      "MSE Epoch 572: 0.008070041102916557\n",
      "MSE Epoch 573: 0.008075386500023872\n",
      "MSE Epoch 574: 0.008074778539813704\n",
      "MSE Epoch 575: 0.008070246741416489\n",
      "MSE Epoch 576: 0.008070069777436217\n",
      "MSE Epoch 577: 0.008072528050392026\n",
      "MSE Epoch 578: 0.008067988208158006\n",
      "MSE Epoch 579: 0.00807380518284898\n",
      "MSE Epoch 580: 0.008073808849791178\n",
      "MSE Epoch 581: 0.00807101774026668\n",
      "MSE Epoch 582: 0.008071809360186795\n",
      "MSE Epoch 583: 0.008067553627988092\n",
      "MSE Epoch 584: 0.008068166266315001\n",
      "MSE Epoch 585: 0.008071634683492208\n",
      "MSE Epoch 586: 0.00806669848008928\n",
      "MSE Epoch 587: 0.008072617487520193\n",
      "MSE Epoch 588: 0.008072908481096723\n",
      "MSE Epoch 589: 0.008069621888243183\n",
      "MSE Epoch 590: 0.008070718247392448\n",
      "MSE Epoch 591: 0.00806621463762457\n",
      "MSE Epoch 592: 0.0080718136192855\n",
      "MSE Epoch 593: 0.0080676669321144\n",
      "MSE Epoch 594: 0.008072290634493058\n",
      "MSE Epoch 595: 0.008068688215811708\n",
      "MSE Epoch 596: 0.0080700411902734\n",
      "MSE Epoch 597: 0.008065320159819315\n",
      "MSE Epoch 598: 0.008070986692620595\n",
      "MSE Epoch 599: 0.008070996384070757\n",
      "MSE Epoch 600: 0.008068183107184414\n",
      "MSE Epoch 601: 0.008069092934115807\n",
      "MSE Epoch 602: 0.008064580497809776\n",
      "MSE Epoch 603: 0.008069674119707387\n",
      "MSE Epoch 604: 0.008066094276151048\n",
      "MSE Epoch 605: 0.008070194119818736\n",
      "MSE Epoch 606: 0.008067116529106378\n",
      "MSE Epoch 607: 0.008068471416783358\n",
      "MSE Epoch 608: 0.008065322895735872\n",
      "MSE Epoch 609: 0.008069915983005141\n",
      "MSE Epoch 610: 0.008068132893982491\n",
      "MSE Epoch 611: 0.008065212688025566\n",
      "MSE Epoch 612: 0.008069147768960139\n",
      "MSE Epoch 613: 0.008066275819999868\n",
      "MSE Epoch 614: 0.008067392272073667\n",
      "MSE Epoch 615: 0.008063714639351165\n",
      "MSE Epoch 616: 0.008063528024666394\n",
      "MSE Epoch 617: 0.008067100967285508\n",
      "MSE Epoch 618: 0.008064066164340125\n",
      "MSE Epoch 619: 0.008068296088084124\n",
      "MSE Epoch 620: 0.008067720317421869\n",
      "MSE Epoch 621: 0.008063591723008958\n",
      "MSE Epoch 622: 0.0080634846295976\n",
      "MSE Epoch 623: 0.008067625852587725\n",
      "MSE Epoch 624: 0.00806389896302604\n",
      "MSE Epoch 625: 0.008067564806310431\n",
      "MSE Epoch 626: 0.00806327133884896\n",
      "MSE Epoch 627: 0.008067405345422279\n",
      "MSE Epoch 628: 0.008066606114739012\n",
      "MSE Epoch 629: 0.008063143284600624\n",
      "MSE Epoch 630: 0.008066645412541307\n",
      "MSE Epoch 631: 0.008063098214689039\n",
      "MSE Epoch 632: 0.008066762298556131\n",
      "MSE Epoch 633: 0.008061734895529428\n",
      "MSE Epoch 634: 0.008066179528470482\n",
      "MSE Epoch 635: 0.00806212972287083\n",
      "MSE Epoch 636: 0.00806601095714132\n",
      "MSE Epoch 637: 0.008061095696493865\n",
      "MSE Epoch 638: 0.008066073977106599\n",
      "MSE Epoch 639: 0.008061837715379673\n",
      "MSE Epoch 640: 0.008065424028378126\n",
      "MSE Epoch 641: 0.008060505842284701\n",
      "MSE Epoch 642: 0.008065186964884268\n",
      "MSE Epoch 643: 0.008060969689625733\n",
      "MSE Epoch 644: 0.008065010272002737\n",
      "MSE Epoch 645: 0.008064247008957756\n",
      "MSE Epoch 646: 0.008061142528914886\n",
      "MSE Epoch 647: 0.00806420050039372\n",
      "MSE Epoch 648: 0.008059758751577403\n",
      "MSE Epoch 649: 0.008064406950084013\n",
      "MSE Epoch 650: 0.008059465884201412\n",
      "MSE Epoch 651: 0.008063811247966946\n",
      "MSE Epoch 652: 0.008059884841723354\n",
      "MSE Epoch 653: 0.008063674780233144\n",
      "MSE Epoch 654: 0.00805984889173567\n",
      "MSE Epoch 655: 0.008063413202030016\n",
      "MSE Epoch 656: 0.008058525514932916\n",
      "MSE Epoch 657: 0.008063182678186646\n",
      "MSE Epoch 658: 0.008058234134913703\n",
      "MSE Epoch 659: 0.008063256469724337\n",
      "MSE Epoch 660: 0.008058726250956374\n",
      "MSE Epoch 661: 0.008062749544243767\n",
      "MSE Epoch 662: 0.008061531149267796\n",
      "MSE Epoch 663: 0.008057697684234033\n",
      "MSE Epoch 664: 0.008062062046870133\n",
      "MSE Epoch 665: 0.008058282160111766\n",
      "MSE Epoch 666: 0.008061802120666959\n",
      "MSE Epoch 667: 0.008056959036572142\n",
      "MSE Epoch 668: 0.008061621365149386\n",
      "MSE Epoch 669: 0.008057501554082412\n",
      "MSE Epoch 670: 0.008061473982674356\n",
      "MSE Epoch 671: 0.008060738064491422\n",
      "MSE Epoch 672: 0.008057733318742332\n",
      "MSE Epoch 673: 0.008060712373477386\n",
      "MSE Epoch 674: 0.008056352590508426\n",
      "MSE Epoch 675: 0.008060942547644381\n",
      "MSE Epoch 676: 0.00805607796796901\n",
      "MSE Epoch 677: 0.008060364153664974\n",
      "MSE Epoch 678: 0.008056502504853564\n",
      "MSE Epoch 679: 0.008060239612803655\n",
      "MSE Epoch 680: 0.00805650532556296\n",
      "MSE Epoch 681: 0.00805999369538549\n",
      "MSE Epoch 682: 0.008055176261010278\n",
      "MSE Epoch 683: 0.008059779743265636\n",
      "MSE Epoch 684: 0.008054897311363437\n",
      "MSE Epoch 685: 0.008059867698291772\n",
      "MSE Epoch 686: 0.008054649963127831\n",
      "MSE Epoch 687: 0.008059256541905833\n",
      "MSE Epoch 688: 0.008055123595481254\n",
      "MSE Epoch 689: 0.008059108457785994\n",
      "MSE Epoch 690: 0.00805792030110897\n",
      "MSE Epoch 691: 0.008055185962159463\n",
      "MSE Epoch 692: 0.008058450544150754\n",
      "MSE Epoch 693: 0.00805364877645187\n",
      "MSE Epoch 694: 0.008058271491339053\n",
      "MSE Epoch 695: 0.008053415406019759\n",
      "MSE Epoch 696: 0.00805838833670937\n",
      "MSE Epoch 697: 0.008054237980096747\n",
      "MSE Epoch 698: 0.008057806265495656\n",
      "MSE Epoch 699: 0.008052937716511407\n",
      "MSE Epoch 700: 0.00805762536776261\n",
      "MSE Epoch 701: 0.008053453070128403\n",
      "MSE Epoch 702: 0.008057477416243186\n",
      "MSE Epoch 703: 0.008056128101221274\n",
      "MSE Epoch 704: 0.00805384826339065\n",
      "MSE Epoch 705: 0.00805597429451987\n",
      "MSE Epoch 706: 0.008052917946084233\n",
      "MSE Epoch 707: 0.008055891219848685\n",
      "MSE Epoch 708: 0.008052651382083985\n",
      "MSE Epoch 709: 0.008055333172443889\n",
      "MSE Epoch 710: 0.008052687876059722\n",
      "MSE Epoch 711: 0.00805624810612488\n",
      "MSE Epoch 712: 0.008051238048563811\n",
      "MSE Epoch 713: 0.008056054833950868\n",
      "MSE Epoch 714: 0.008051765952816326\n",
      "MSE Epoch 715: 0.008055890119083597\n",
      "MSE Epoch 716: 0.008055156685462783\n",
      "MSE Epoch 717: 0.008052044567131121\n",
      "MSE Epoch 718: 0.008055129185870874\n",
      "MSE Epoch 719: 0.008050687588674662\n",
      "MSE Epoch 720: 0.008055361337038675\n",
      "MSE Epoch 721: 0.00805044606373954\n",
      "MSE Epoch 722: 0.008054796278677764\n",
      "MSE Epoch 723: 0.008051229545017215\n",
      "MSE Epoch 724: 0.00805456981356877\n",
      "MSE Epoch 725: 0.00804988195771681\n",
      "MSE Epoch 726: 0.0080544060437279\n",
      "MSE Epoch 727: 0.008050393758864632\n",
      "MSE Epoch 728: 0.008054270080916548\n",
      "MSE Epoch 729: 0.008053142429820293\n",
      "MSE Epoch 730: 0.008050543095005483\n",
      "MSE Epoch 731: 0.008053627540427646\n",
      "MSE Epoch 732: 0.008048974208378083\n",
      "MSE Epoch 733: 0.008053503884666814\n",
      "MSE Epoch 734: 0.008048748140722567\n",
      "MSE Epoch 735: 0.008053274013572297\n",
      "MSE Epoch 736: 0.0080492631609017\n",
      "MSE Epoch 737: 0.008053143335201544\n",
      "MSE Epoch 738: 0.008052442765189204\n",
      "MSE Epoch 739: 0.008049563548018274\n",
      "MSE Epoch 740: 0.008052430792787084\n",
      "MSE Epoch 741: 0.008048203896680407\n",
      "MSE Epoch 742: 0.008052324154670809\n",
      "MSE Epoch 743: 0.008048959830849047\n",
      "MSE Epoch 744: 0.008052122850783588\n",
      "MSE Epoch 745: 0.008047619513103023\n",
      "MSE Epoch 746: 0.008052335592314707\n",
      "MSE Epoch 747: 0.008047391932719758\n",
      "MSE Epoch 748: 0.008051770857175124\n",
      "MSE Epoch 749: 0.008047863725773817\n",
      "MSE Epoch 750: 0.008051647526466903\n",
      "MSE Epoch 751: 0.008050578055069877\n",
      "MSE Epoch 752: 0.008048038797820954\n",
      "MSE Epoch 753: 0.00805105231659224\n",
      "MSE Epoch 754: 0.008046440881797776\n",
      "MSE Epoch 755: 0.0080509087203927\n",
      "MSE Epoch 756: 0.008046220786831457\n",
      "MSE Epoch 757: 0.008050691206056374\n",
      "MSE Epoch 758: 0.008047032496910451\n",
      "MSE Epoch 759: 0.008050469959523247\n",
      "MSE Epoch 760: 0.008045728194391802\n",
      "MSE Epoch 761: 0.008050319238006773\n",
      "MSE Epoch 762: 0.008046303292118794\n",
      "MSE Epoch 763: 0.008050184727903469\n",
      "MSE Epoch 764: 0.008048984709140285\n",
      "MSE Epoch 765: 0.008046500092868959\n",
      "MSE Epoch 766: 0.00804956700615314\n",
      "MSE Epoch 767: 0.008044942988609272\n",
      "MSE Epoch 768: 0.00804945109166734\n",
      "MSE Epoch 769: 0.008044732114507847\n",
      "MSE Epoch 770: 0.00804923103351369\n",
      "MSE Epoch 771: 0.008045544756929294\n",
      "MSE Epoch 772: 0.00804900912341627\n",
      "MSE Epoch 773: 0.008044257971442774\n",
      "MSE Epoch 774: 0.008048859063742123\n",
      "MSE Epoch 775: 0.00804479631561883\n",
      "MSE Epoch 776: 0.008048727224257434\n",
      "MSE Epoch 777: 0.008047476237727889\n",
      "MSE Epoch 778: 0.008045049239744027\n",
      "MSE Epoch 779: 0.008048112621003023\n",
      "MSE Epoch 780: 0.008043497706764855\n",
      "MSE Epoch 781: 0.008047997950099677\n",
      "MSE Epoch 782: 0.008043292218470827\n",
      "MSE Epoch 783: 0.008047780592653228\n",
      "MSE Epoch 784: 0.008044110642139397\n",
      "MSE Epoch 785: 0.008047560493617314\n",
      "MSE Epoch 786: 0.008042827394200184\n",
      "MSE Epoch 787: 0.008047414003624895\n",
      "MSE Epoch 788: 0.008043368503348228\n",
      "MSE Epoch 789: 0.008047283839664769\n",
      "MSE Epoch 790: 0.008046000740159981\n",
      "MSE Epoch 791: 0.008043633232718868\n",
      "MSE Epoch 792: 0.00804667778998582\n",
      "MSE Epoch 793: 0.008042081978999287\n",
      "MSE Epoch 794: 0.008046565636719703\n",
      "MSE Epoch 795: 0.008041880683461977\n",
      "MSE Epoch 796: 0.008046351947335653\n",
      "MSE Epoch 797: 0.008042705899794717\n",
      "MSE Epoch 798: 0.008046134341444452\n",
      "MSE Epoch 799: 0.00804142334217011\n",
      "MSE Epoch 800: 0.008045992104675588\n",
      "MSE Epoch 801: 0.008041968062869288\n",
      "MSE Epoch 802: 0.008045864145664113\n",
      "MSE Epoch 803: 0.008044556394869415\n",
      "MSE Epoch 804: 0.00804224328834311\n",
      "MSE Epoch 805: 0.008045266886937267\n",
      "MSE Epoch 806: 0.00804069088838471\n",
      "MSE Epoch 807: 0.008045157620815237\n",
      "MSE Epoch 808: 0.008040493493433689\n",
      "MSE Epoch 809: 0.008044947814837838\n",
      "MSE Epoch 810: 0.008041325927797762\n",
      "MSE Epoch 811: 0.00804473284229972\n",
      "MSE Epoch 812: 0.008040043142507548\n",
      "MSE Epoch 813: 0.008044594944461398\n",
      "MSE Epoch 814: 0.008040592172352215\n",
      "MSE Epoch 815: 0.008044469261994642\n",
      "MSE Epoch 816: 0.008043366754491361\n",
      "MSE Epoch 817: 0.008040473121135594\n",
      "MSE Epoch 818: 0.008043161902243467\n",
      "MSE Epoch 819: 0.008041279590993823\n",
      "MSE Epoch 820: 0.008043022038549561\n",
      "MSE Epoch 821: 0.008040083846651043\n",
      "MSE Epoch 822: 0.008044065051956149\n",
      "MSE Epoch 823: 0.008042440345380042\n",
      "MSE Epoch 824: 0.008039769969094186\n",
      "MSE Epoch 825: 0.008042169801346246\n",
      "MSE Epoch 826: 0.00804055483298577\n",
      "MSE Epoch 827: 0.00804201684484996\n",
      "MSE Epoch 828: 0.00803918536240733\n",
      "MSE Epoch 829: 0.008041920513700175\n",
      "MSE Epoch 830: 0.008038959832574601\n",
      "MSE Epoch 831: 0.008041745218735546\n",
      "MSE Epoch 832: 0.008038717759051243\n",
      "MSE Epoch 833: 0.008041571510469926\n",
      "MSE Epoch 834: 0.008038568967933776\n",
      "MSE Epoch 835: 0.008042025162096823\n",
      "MSE Epoch 836: 0.008038359358733543\n",
      "MSE Epoch 837: 0.008041710130885704\n",
      "MSE Epoch 838: 0.008041296232669724\n",
      "MSE Epoch 839: 0.008040506039399068\n",
      "MSE Epoch 840: 0.008038098506800853\n",
      "MSE Epoch 841: 0.00804132646563639\n",
      "MSE Epoch 842: 0.008036881403226104\n",
      "MSE Epoch 843: 0.008040842182490734\n",
      "MSE Epoch 844: 0.008036626852216337\n",
      "MSE Epoch 845: 0.00804065442224213\n",
      "MSE Epoch 846: 0.00803715057128994\n",
      "MSE Epoch 847: 0.008040558678835954\n",
      "MSE Epoch 848: 0.008037193679891674\n",
      "MSE Epoch 849: 0.008040350909824063\n",
      "MSE Epoch 850: 0.008035942255191903\n",
      "MSE Epoch 851: 0.008037922012794997\n",
      "MSE Epoch 852: 0.008040965959433169\n",
      "MSE Epoch 853: 0.008037375307350929\n",
      "MSE Epoch 854: 0.008040163033802705\n",
      "MSE Epoch 855: 0.008035495343746358\n",
      "MSE Epoch 856: 0.008039807835120056\n",
      "MSE Epoch 857: 0.00803523319349936\n",
      "MSE Epoch 858: 0.008039540779804157\n",
      "MSE Epoch 859: 0.00803575480444672\n",
      "MSE Epoch 860: 0.008039398772810716\n",
      "MSE Epoch 861: 0.008038273375074791\n",
      "MSE Epoch 862: 0.008038548786510237\n",
      "MSE Epoch 863: 0.008035869412613222\n",
      "MSE Epoch 864: 0.008038514358918227\n",
      "MSE Epoch 865: 0.008034598834619188\n",
      "MSE Epoch 866: 0.008035708092092787\n",
      "MSE Epoch 867: 0.00803963659082583\n",
      "MSE Epoch 868: 0.00803668286299369\n",
      "MSE Epoch 869: 0.00803776718199069\n",
      "MSE Epoch 870: 0.008034503490380698\n",
      "MSE Epoch 871: 0.00803871020491213\n",
      "MSE Epoch 872: 0.008033951101395825\n",
      "MSE Epoch 873: 0.008037933370565936\n",
      "MSE Epoch 874: 0.00803441141647681\n",
      "MSE Epoch 875: 0.008037805251679422\n",
      "MSE Epoch 876: 0.0080344783495228\n",
      "MSE Epoch 877: 0.008037582864922904\n",
      "MSE Epoch 878: 0.00803320737773506\n",
      "MSE Epoch 879: 0.008037650669530858\n",
      "MSE Epoch 880: 0.008036512099224334\n",
      "MSE Epoch 881: 0.008033666989327276\n",
      "MSE Epoch 882: 0.008036426872510865\n",
      "MSE Epoch 883: 0.008032540829205997\n",
      "MSE Epoch 884: 0.008034643044411274\n",
      "MSE Epoch 885: 0.00803801861083896\n",
      "MSE Epoch 886: 0.008034811225532465\n",
      "MSE Epoch 887: 0.00803622501445055\n",
      "MSE Epoch 888: 0.008033215629118752\n",
      "MSE Epoch 889: 0.008036043715813065\n",
      "MSE Epoch 890: 0.008033064477384503\n",
      "MSE Epoch 891: 0.008036475599717013\n",
      "MSE Epoch 892: 0.008032841828834491\n",
      "MSE Epoch 893: 0.008036159981686405\n",
      "MSE Epoch 894: 0.008035520800975707\n",
      "MSE Epoch 895: 0.008031725586793782\n",
      "MSE Epoch 896: 0.008032755844265629\n",
      "MSE Epoch 897: 0.008036624513538006\n",
      "MSE Epoch 898: 0.008033256757759757\n",
      "MSE Epoch 899: 0.008035603883669747\n",
      "MSE Epoch 900: 0.008031424764071078\n",
      "MSE Epoch 901: 0.008035430470691577\n",
      "MSE Epoch 902: 0.008031760675428353\n",
      "MSE Epoch 903: 0.008035209333601105\n",
      "MSE Epoch 904: 0.008031833047436118\n",
      "MSE Epoch 905: 0.008034927012306773\n",
      "MSE Epoch 906: 0.008030559768967908\n",
      "MSE Epoch 907: 0.008034758544534126\n",
      "MSE Epoch 908: 0.00803034728985492\n",
      "MSE Epoch 909: 0.00803177142077975\n",
      "MSE Epoch 910: 0.008035673543001174\n",
      "MSE Epoch 911: 0.008032462302468442\n",
      "MSE Epoch 912: 0.008033911356453144\n",
      "MSE Epoch 913: 0.008031077447742013\n",
      "MSE Epoch 914: 0.00803446265087111\n",
      "MSE Epoch 915: 0.008033366334272332\n",
      "MSE Epoch 916: 0.008030587024187808\n",
      "MSE Epoch 917: 0.008032822009309947\n",
      "MSE Epoch 918: 0.008030299060436473\n",
      "MSE Epoch 919: 0.008032753829161333\n",
      "MSE Epoch 920: 0.00803003052315375\n",
      "MSE Epoch 921: 0.008032853347730011\n",
      "MSE Epoch 922: 0.008030234629935889\n",
      "MSE Epoch 923: 0.008034208924104788\n",
      "MSE Epoch 924: 0.008031501114638552\n",
      "MSE Epoch 925: 0.008032489441319688\n",
      "MSE Epoch 926: 0.008029052227626255\n",
      "MSE Epoch 927: 0.008033457896901991\n",
      "MSE Epoch 928: 0.008029769505355333\n",
      "MSE Epoch 929: 0.008032698182348811\n",
      "MSE Epoch 930: 0.008028437007905473\n",
      "MSE Epoch 931: 0.008032533788999752\n",
      "MSE Epoch 932: 0.00802822330560428\n",
      "MSE Epoch 933: 0.008029496784598657\n",
      "MSE Epoch 934: 0.008033405638319821\n",
      "MSE Epoch 935: 0.008030354238259934\n",
      "MSE Epoch 936: 0.008031691560315619\n",
      "MSE Epoch 937: 0.008027893466213442\n",
      "MSE Epoch 938: 0.008032292439104445\n",
      "MSE Epoch 939: 0.00803143656859328\n",
      "MSE Epoch 940: 0.008028771019758818\n",
      "MSE Epoch 941: 0.008031422880950255\n",
      "MSE Epoch 942: 0.008027441714457503\n",
      "MSE Epoch 943: 0.008028917973434605\n",
      "MSE Epoch 944: 0.008031889534973606\n",
      "MSE Epoch 945: 0.008028929986094958\n",
      "MSE Epoch 946: 0.008031370534664224\n",
      "MSE Epoch 947: 0.008026105886455293\n",
      "MSE Epoch 948: 0.008031202054823439\n",
      "MSE Epoch 949: 0.008026706086246612\n",
      "MSE Epoch 950: 0.008030880128603807\n",
      "MSE Epoch 951: 0.008027278465534431\n",
      "MSE Epoch 952: 0.008027694112069675\n",
      "MSE Epoch 953: 0.00803182430570615\n",
      "MSE Epoch 954: 0.008027045159236659\n",
      "MSE Epoch 955: 0.008031162533783576\n",
      "MSE Epoch 956: 0.008029776993751687\n",
      "MSE Epoch 957: 0.008026397346936552\n",
      "MSE Epoch 958: 0.008030614130898818\n",
      "MSE Epoch 959: 0.008027090793679541\n",
      "MSE Epoch 960: 0.008029883621826916\n",
      "MSE Epoch 961: 0.00802568047280587\n",
      "MSE Epoch 962: 0.008026880223887143\n",
      "MSE Epoch 963: 0.008030729177678846\n",
      "MSE Epoch 964: 0.008027881878314895\n",
      "MSE Epoch 965: 0.008029160934704975\n",
      "MSE Epoch 966: 0.008024692512102251\n",
      "MSE Epoch 967: 0.008030064659896882\n",
      "MSE Epoch 968: 0.008026348988983096\n",
      "MSE Epoch 969: 0.00802937505609109\n",
      "MSE Epoch 970: 0.008024912735147369\n",
      "MSE Epoch 971: 0.008029090779405317\n",
      "MSE Epoch 972: 0.008024689224835724\n",
      "MSE Epoch 973: 0.008025980053145032\n",
      "MSE Epoch 974: 0.008029927322120323\n",
      "MSE Epoch 975: 0.008025177434698804\n",
      "MSE Epoch 976: 0.008028967171514684\n",
      "MSE Epoch 977: 0.008027752131223592\n",
      "MSE Epoch 978: 0.008025358949597796\n",
      "MSE Epoch 979: 0.008028241911438307\n",
      "MSE Epoch 980: 0.008025159158726475\n",
      "MSE Epoch 981: 0.00802510182454971\n",
      "MSE Epoch 982: 0.00802918125616499\n",
      "MSE Epoch 983: 0.008024460929826003\n",
      "MSE Epoch 984: 0.008028566933946124\n",
      "MSE Epoch 985: 0.008022959876275498\n",
      "MSE Epoch 986: 0.008028041916694722\n",
      "MSE Epoch 987: 0.008024793366609563\n",
      "MSE Epoch 988: 0.008027737757514185\n",
      "MSE Epoch 989: 0.008023316848791545\n",
      "MSE Epoch 990: 0.00802444139444433\n",
      "MSE Epoch 991: 0.00802842273334782\n",
      "MSE Epoch 992: 0.008025397099301558\n",
      "MSE Epoch 993: 0.008026883842638164\n",
      "MSE Epoch 994: 0.008022991518966066\n",
      "MSE Epoch 995: 0.008027902608995371\n",
      "MSE Epoch 996: 0.008026308665206202\n",
      "MSE Epoch 997: 0.008024793688895004\n",
      "MSE Epoch 998: 0.008026021418429705\n",
      "MSE Epoch 999: 0.00802345928953122\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x25c4581cc10>]"
      ]
     },
     "execution_count": 750,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class MyMLP():\n",
    "    def __init__(self, n_inputs, n_outputs, verbose=False):\n",
    "        self.fitted = False\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        self.verbose=verbose\n",
    "        self.n_hidden = []\n",
    "        self.weights = []\n",
    "        self.activation_functions = [Identity(), Sigmoid()]\n",
    "        self.layer_inputs = []\n",
    "        self.layer_outputs = []\n",
    "        self.deltas = []\n",
    "\n",
    "    def get_n_neurons_of_last_layer(self):\n",
    "        if len(self.weights) == 0:\n",
    "            return self.n_inputs\n",
    "        else:\n",
    "            return len(self.weights[-1])\n",
    "\n",
    "    def add_hidden_layer(self, n_neurons, activation_function):\n",
    "        self.n_hidden.append(n_neurons)\n",
    "        self.activation_functions = self.activation_functions[:-1] \\\n",
    "            + [activation_function] \\\n",
    "            + [self.activation_functions[-1]]\n",
    "\n",
    "    def initialize_weights(self, random_state=8776123):\n",
    "        self.weights = []\n",
    "\n",
    "        seed = np.random.RandomState(random_state)\n",
    "        self.layers = [self.n_inputs] + self.n_hidden + [self.n_outputs]\n",
    "        \n",
    "        for i in range(len(self.layers)-1):\n",
    "            # Initialization strategies\n",
    "            if type(self.activation_functions[i]) == ReLU:\n",
    "                w = seed.normal(\n",
    "                    size = (self.layers[i+1], self.layers[i])\n",
    "                ) * np.sqrt(2/self.layers[i])  \n",
    "                # w = np.random.rand(self.layers[i+1], self.layers[i])\n",
    "            else:\n",
    "                w = seed.normal(\n",
    "                    size = (self.layers[i+1], self.layers[i])\n",
    "                ) * np.sqrt(1/self.layers[i])\n",
    "                # w = np.random.rand(self.layers[i+1], self.layers[i])\n",
    "            self.weights.append(w)\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        activated_values = X\n",
    "\n",
    "        self.layer_inputs = []\n",
    "        self.layer_outputs = []\n",
    "\n",
    "        self.layer_outputs.append(activated_values)\n",
    "\n",
    "        for i, w in enumerate(self.weights):\n",
    "            # Insert first column with ones to multiply bias term\n",
    "            activation_function = self.activation_functions[i+1]            \n",
    "            \n",
    "            # Calculating input of hidden layer\n",
    "            hidden_input = np.array(activated_values @ w.T)\n",
    "            if activation_function is not None:\n",
    "                activated_values = activation_function.get_value(hidden_input)\n",
    "            else:\n",
    "                activated_values = hidden_input\n",
    "                \n",
    "            self.layer_inputs.append(hidden_input)\n",
    "            self.layer_outputs.append(activated_values)\n",
    "        \n",
    "        self.layer_inputs = [X] + self.layer_inputs\n",
    "        return activated_values # Output\n",
    "\n",
    "\n",
    "    def back_propagation(self, error, alpha=0.1):\n",
    "        # calcular os deltas\n",
    "        last_layers = True\n",
    "        self.deltas = []\n",
    "        for i in range(len(self.activation_functions)-1, 0, -1):\n",
    "            act_function = self.activation_functions[i]\n",
    "            layer_inputs = self.layer_inputs[i]\n",
    "            if last_layers:\n",
    "                delta_i = error * act_function.get_derivative(layer_inputs)\n",
    "                self.deltas = [delta_i] + self.deltas\n",
    "                last_layers=False\n",
    "            else:\n",
    "                weights = self.weights[i]\n",
    "                \n",
    "                if self.verbose == True:\n",
    "                    print(f'------ ITERAÃÃO {i} ------')\n",
    "                    print('\\nweights')\n",
    "                    print(weights.shape)\n",
    "                    print(weights)\n",
    "                    print('\\ndelta')\n",
    "                    print(self.deltas[0].shape)\n",
    "                    print(self.deltas[0])\n",
    "                    print('\\nact_function')\n",
    "                    print(act_function.get_derivative(layer_inputs).shape)\n",
    "                    print(act_function.get_derivative(layer_inputs))\n",
    "\n",
    "                delta_i = act_function.get_derivative(layer_inputs) \\\n",
    "                    * (self.deltas[0] @ weights)\n",
    "                self.deltas = [delta_i] + self.deltas\n",
    "\n",
    "        for i in range(len(self.weights)):\n",
    "            gradient = self.deltas[i].T @ self.layer_outputs[i]\n",
    "            self.weights[i] = self.weights[i] - alpha * gradient\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        activated_values = X\n",
    "\n",
    "        for i, w in enumerate(self.weights):\n",
    "            activation_function = self.activation_functions[i+1]           \n",
    "            \n",
    "            # Calculating input of hidden layer\n",
    "            hidden_input = np.array(activated_values @ w.T)\n",
    "            activated_values = activation_function.get_value(hidden_input)\n",
    "\n",
    "        \n",
    "        self.layer_inputs = [X] + self.layer_inputs\n",
    "        return activated_values\n",
    "\n",
    "my_mlp = MyMLP(n_inputs=8, n_outputs=1, verbose=False)\n",
    "my_mlp.add_hidden_layer(n_neurons=4, activation_function=ReLU())\n",
    "# my_mlp.add_hidden_layer(n_neurons=2, activation_function=ReLU())\n",
    "\n",
    "my_mlp.initialize_weights(65421)\n",
    "\n",
    "cost_function_curve = []\n",
    "\n",
    "epochs = 1000\n",
    "for i in range(epochs):\n",
    "    mse = 0\n",
    "    count = 0\n",
    "    for input, output in zip(X_train_scaled, y_train_scaled):\n",
    "        input_reshaped = input.reshape(1, -1)\n",
    "        output_reshaped = output.reshape(1, -1)        \n",
    "        y_pred = my_mlp.forward_propagation(input_reshaped)\n",
    "        error = y_pred - output_reshaped\n",
    "        my_mlp.back_propagation(error, 0.1)\n",
    "\n",
    "        mse+=get_mse(output, y_pred)\n",
    "        count+=1\n",
    "\n",
    "    cost_function_curve.append(mse/count)\n",
    "    print(f'MSE Epoch {i}:', mse/count)\n",
    "\n",
    "# for i in range(epochs):\n",
    " \n",
    "#     y_pred = my_mlp.forward_propagation(X_train_scaled)\n",
    "#     error = y_pred - y_train_scaled\n",
    "#     my_mlp.back_propagation(error, 0.1)\n",
    "\n",
    "#     mse=get_mse(y_train_scaled, y_pred)\n",
    "\n",
    "#     cost_function_curve.append(mse)\n",
    "#     print(f'MSE Epoch {i}:', mse)\n",
    "\n",
    "\n",
    "plt.plot(cost_function_curve)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01158037522566841"
      ]
     },
     "execution_count": 747,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred = my_mlp.predict(X_test_scaled)\n",
    "\n",
    "mse = get_mse(\n",
    "    y_test_scaled,\n",
    "    y_test_pred\n",
    ")\n",
    "mse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teste Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input\n",
    "import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "618/618 [==============================] - 1s 760us/step - loss: 0.0605\n",
      "Epoch 2/100\n",
      "618/618 [==============================] - 0s 760us/step - loss: 0.0566\n",
      "Epoch 3/100\n",
      "618/618 [==============================] - 1s 835us/step - loss: 0.0573\n",
      "Epoch 4/100\n",
      "618/618 [==============================] - 0s 793us/step - loss: 0.0540\n",
      "Epoch 5/100\n",
      "618/618 [==============================] - 1s 891us/step - loss: 0.0569\n",
      "Epoch 6/100\n",
      "618/618 [==============================] - 0s 733us/step - loss: 0.0559\n",
      "Epoch 7/100\n",
      "618/618 [==============================] - 0s 764us/step - loss: 0.0563\n",
      "Epoch 8/100\n",
      "618/618 [==============================] - 0s 759us/step - loss: 0.0566\n",
      "Epoch 9/100\n",
      "618/618 [==============================] - 0s 768us/step - loss: 0.0576\n",
      "Epoch 10/100\n",
      "618/618 [==============================] - 0s 762us/step - loss: 0.0559\n",
      "Epoch 11/100\n",
      "618/618 [==============================] - 0s 768us/step - loss: 0.0563\n",
      "Epoch 12/100\n",
      "618/618 [==============================] - 0s 754us/step - loss: 0.0568\n",
      "Epoch 13/100\n",
      "618/618 [==============================] - 0s 767us/step - loss: 0.0570\n",
      "Epoch 14/100\n",
      "618/618 [==============================] - 0s 756us/step - loss: 0.0572\n",
      "Epoch 15/100\n",
      "618/618 [==============================] - 0s 757us/step - loss: 0.0570\n",
      "Epoch 16/100\n",
      "618/618 [==============================] - 0s 765us/step - loss: 0.0559\n",
      "Epoch 17/100\n",
      "618/618 [==============================] - 0s 762us/step - loss: 0.0584\n",
      "Epoch 18/100\n",
      "618/618 [==============================] - 0s 762us/step - loss: 0.0582\n",
      "Epoch 19/100\n",
      "618/618 [==============================] - 0s 756us/step - loss: 0.0574\n",
      "Epoch 20/100\n",
      "618/618 [==============================] - 1s 832us/step - loss: 0.0581\n",
      "Epoch 21/100\n",
      "618/618 [==============================] - 0s 761us/step - loss: 0.0567\n",
      "Epoch 22/100\n",
      "618/618 [==============================] - 0s 760us/step - loss: 0.0556\n",
      "Epoch 23/100\n",
      "618/618 [==============================] - 0s 783us/step - loss: 0.0566\n",
      "Epoch 24/100\n",
      "618/618 [==============================] - 0s 777us/step - loss: 0.0560\n",
      "Epoch 25/100\n",
      "618/618 [==============================] - 1s 838us/step - loss: 0.0568\n",
      "Epoch 26/100\n",
      "618/618 [==============================] - 1s 829us/step - loss: 0.0579\n",
      "Epoch 27/100\n",
      "618/618 [==============================] - 1s 860us/step - loss: 0.0552\n",
      "Epoch 28/100\n",
      "618/618 [==============================] - 0s 765us/step - loss: 0.0568\n",
      "Epoch 29/100\n",
      "618/618 [==============================] - 0s 778us/step - loss: 0.0559\n",
      "Epoch 30/100\n",
      "618/618 [==============================] - 0s 779us/step - loss: 0.0571\n",
      "Epoch 31/100\n",
      "618/618 [==============================] - 0s 763us/step - loss: 0.0573\n",
      "Epoch 32/100\n",
      "618/618 [==============================] - 0s 759us/step - loss: 0.0577\n",
      "Epoch 33/100\n",
      "618/618 [==============================] - 0s 757us/step - loss: 0.0556\n",
      "Epoch 34/100\n",
      "618/618 [==============================] - 0s 772us/step - loss: 0.0579\n",
      "Epoch 35/100\n",
      "618/618 [==============================] - 0s 748us/step - loss: 0.0564\n",
      "Epoch 36/100\n",
      "618/618 [==============================] - 1s 828us/step - loss: 0.0558\n",
      "Epoch 37/100\n",
      "618/618 [==============================] - 0s 752us/step - loss: 0.0571\n",
      "Epoch 38/100\n",
      "618/618 [==============================] - 0s 783us/step - loss: 0.0554\n",
      "Epoch 39/100\n",
      "618/618 [==============================] - 0s 753us/step - loss: 0.0561\n",
      "Epoch 40/100\n",
      "618/618 [==============================] - 0s 769us/step - loss: 0.0585\n",
      "Epoch 41/100\n",
      "618/618 [==============================] - 0s 775us/step - loss: 0.0569\n",
      "Epoch 42/100\n",
      "618/618 [==============================] - 1s 817us/step - loss: 0.0537\n",
      "Epoch 43/100\n",
      "618/618 [==============================] - 1s 809us/step - loss: 0.0573\n",
      "Epoch 44/100\n",
      "618/618 [==============================] - 1s 944us/step - loss: 0.0579\n",
      "Epoch 45/100\n",
      "618/618 [==============================] - 1s 822us/step - loss: 0.0566\n",
      "Epoch 46/100\n",
      "618/618 [==============================] - 0s 793us/step - loss: 0.0552\n",
      "Epoch 47/100\n",
      "618/618 [==============================] - 0s 805us/step - loss: 0.0574\n",
      "Epoch 48/100\n",
      "618/618 [==============================] - 1s 812us/step - loss: 0.0572\n",
      "Epoch 49/100\n",
      "618/618 [==============================] - 0s 777us/step - loss: 0.0551\n",
      "Epoch 50/100\n",
      "618/618 [==============================] - 0s 768us/step - loss: 0.0582\n",
      "Epoch 51/100\n",
      "618/618 [==============================] - 0s 781us/step - loss: 0.0563\n",
      "Epoch 52/100\n",
      "618/618 [==============================] - 0s 781us/step - loss: 0.0572\n",
      "Epoch 53/100\n",
      "618/618 [==============================] - 0s 767us/step - loss: 0.0573\n",
      "Epoch 54/100\n",
      "618/618 [==============================] - 0s 790us/step - loss: 0.0564\n",
      "Epoch 55/100\n",
      "618/618 [==============================] - 0s 786us/step - loss: 0.0570\n",
      "Epoch 56/100\n",
      "618/618 [==============================] - 0s 802us/step - loss: 0.0571\n",
      "Epoch 57/100\n",
      "618/618 [==============================] - 0s 792us/step - loss: 0.0559\n",
      "Epoch 58/100\n",
      "618/618 [==============================] - 0s 775us/step - loss: 0.0562\n",
      "Epoch 59/100\n",
      "618/618 [==============================] - 0s 772us/step - loss: 0.0567\n",
      "Epoch 60/100\n",
      "618/618 [==============================] - 0s 764us/step - loss: 0.0580\n",
      "Epoch 61/100\n",
      "618/618 [==============================] - 0s 765us/step - loss: 0.0570\n",
      "Epoch 62/100\n",
      "618/618 [==============================] - 1s 867us/step - loss: 0.0565\n",
      "Epoch 63/100\n",
      "618/618 [==============================] - 0s 765us/step - loss: 0.0559\n",
      "Epoch 64/100\n",
      "618/618 [==============================] - 0s 781us/step - loss: 0.0555\n",
      "Epoch 65/100\n",
      "618/618 [==============================] - 0s 776us/step - loss: 0.0565\n",
      "Epoch 66/100\n",
      "618/618 [==============================] - 0s 771us/step - loss: 0.0571\n",
      "Epoch 67/100\n",
      "618/618 [==============================] - 0s 792us/step - loss: 0.0572\n",
      "Epoch 68/100\n",
      "618/618 [==============================] - 0s 782us/step - loss: 0.0564\n",
      "Epoch 69/100\n",
      "618/618 [==============================] - 0s 772us/step - loss: 0.0570\n",
      "Epoch 70/100\n",
      "618/618 [==============================] - 0s 765us/step - loss: 0.0575\n",
      "Epoch 71/100\n",
      "618/618 [==============================] - 1s 851us/step - loss: 0.0565\n",
      "Epoch 72/100\n",
      "618/618 [==============================] - 0s 799us/step - loss: 0.0562\n",
      "Epoch 73/100\n",
      "618/618 [==============================] - 1s 827us/step - loss: 0.0562\n",
      "Epoch 74/100\n",
      "618/618 [==============================] - 0s 745us/step - loss: 0.0573\n",
      "Epoch 75/100\n",
      "618/618 [==============================] - 0s 755us/step - loss: 0.0574\n",
      "Epoch 76/100\n",
      "618/618 [==============================] - 0s 771us/step - loss: 0.0578\n",
      "Epoch 77/100\n",
      "618/618 [==============================] - 1s 909us/step - loss: 0.0564\n",
      "Epoch 78/100\n",
      "618/618 [==============================] - 0s 803us/step - loss: 0.0573\n",
      "Epoch 79/100\n",
      "618/618 [==============================] - 0s 754us/step - loss: 0.0565\n",
      "Epoch 80/100\n",
      "618/618 [==============================] - 0s 768us/step - loss: 0.0565\n",
      "Epoch 81/100\n",
      "618/618 [==============================] - 0s 767us/step - loss: 0.0572\n",
      "Epoch 82/100\n",
      "618/618 [==============================] - 0s 749us/step - loss: 0.0561\n",
      "Epoch 83/100\n",
      "618/618 [==============================] - 0s 754us/step - loss: 0.0573\n",
      "Epoch 84/100\n",
      "618/618 [==============================] - 0s 746us/step - loss: 0.0560\n",
      "Epoch 85/100\n",
      "618/618 [==============================] - 0s 761us/step - loss: 0.0569\n",
      "Epoch 86/100\n",
      "618/618 [==============================] - 0s 746us/step - loss: 0.0567\n",
      "Epoch 87/100\n",
      "618/618 [==============================] - 0s 793us/step - loss: 0.0573\n",
      "Epoch 88/100\n",
      "618/618 [==============================] - 0s 760us/step - loss: 0.0567\n",
      "Epoch 89/100\n",
      "618/618 [==============================] - 1s 962us/step - loss: 0.0574\n",
      "Epoch 90/100\n",
      "618/618 [==============================] - 0s 781us/step - loss: 0.0569\n",
      "Epoch 91/100\n",
      "618/618 [==============================] - 0s 772us/step - loss: 0.0575\n",
      "Epoch 92/100\n",
      "618/618 [==============================] - 0s 762us/step - loss: 0.0571\n",
      "Epoch 93/100\n",
      "618/618 [==============================] - 1s 842us/step - loss: 0.0548\n",
      "Epoch 94/100\n",
      "618/618 [==============================] - 0s 739us/step - loss: 0.0559\n",
      "Epoch 95/100\n",
      "618/618 [==============================] - 0s 757us/step - loss: 0.0572\n",
      "Epoch 96/100\n",
      "618/618 [==============================] - 0s 776us/step - loss: 0.0566\n",
      "Epoch 97/100\n",
      "618/618 [==============================] - 0s 746us/step - loss: 0.0581\n",
      "Epoch 98/100\n",
      "618/618 [==============================] - 0s 760us/step - loss: 0.0572\n",
      "Epoch 99/100\n",
      "618/618 [==============================] - 0s 755us/step - loss: 0.0568\n",
      "Epoch 100/100\n",
      "618/618 [==============================] - 0s 760us/step - loss: 0.0566\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25c40aeaaf0>"
      ]
     },
     "execution_count": 751,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(8))\n",
    "model.add(Dense(4, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mse', optimizer=tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.0))\n",
    "\n",
    "model.fit(X_train_scaled, y_train_scaled, epochs=100, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 833us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06007824461022472"
      ]
     },
     "execution_count": 752,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "mse = get_mse(\n",
    "    y_test_scaled,\n",
    "    y_test_pred\n",
    ")\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_mlp.deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.35, 0.9 ],\n",
       "       [0.35, 0.9 ]])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_mlp.layer_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.00265449],\n",
       "        [0.00817165]]),\n",
       " array([[0.04068113]])]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_mlp.deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.35, 0.9 ]]), array([[0.755, 0.68 ]]), array([[0.6981492]])]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_mlp.layer_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.35, 0.9 ]]), array([[0.755, 0.68 ]]), array([[0.8385]])]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_mlp.layer_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.35, 0.9 ]]), array([[0.6802672, 0.6637387]]), array([[0.69028349]])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_mlp.hidden_activation_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1002,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.35, 0.9 ]]), array([[0.755, 0.68 ]]), array([[0.80144499]])]"
      ]
     },
     "execution_count": 1002,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_mlp.layer_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1003,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.35, 0.9 ]]), array([[0.6802672, 0.6637387]]), array([[0.69028349]])]"
      ]
     },
     "execution_count": 1003,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_mlp.hidden_activation_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 961,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.01      ,  0.01      ],\n",
       "        [-1.30874051, -1.02080529],\n",
       "        [-0.73480594,  1.48559353]]),\n",
       " array([[ 0.        ],\n",
       "        [-0.96229814],\n",
       "        [-0.25849241]])]"
      ]
     },
     "execution_count": 961,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_mlp.hidden_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 922,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[1, 1]]), array([[-2.03354645,  0.47478824]]), array([[-0.12272916]])]"
      ]
     },
     "execution_count": 922,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_mlp.layer_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 920,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[1, 1]]), array([[0.        , 0.47478824]]), array([[-0.12272916]])]"
      ]
     },
     "execution_count": 920,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_mlp.hidden_activation_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-4.0770929 ,  0.93957648]]), array([[-0.24287339]])]"
      ]
     },
     "execution_count": 865,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[2, 2]]),\n",
       " array([[0.        , 0.        , 2.25006011]]),\n",
       " array([[1.72497389]])]"
      ]
     },
     "execution_count": 834,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_mlp.hidden_activation_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.55005223],\n",
       "        [-0.55005223]]),\n",
       " array([[ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [-0.61882529]])]"
      ]
     },
     "execution_count": 829,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_mlp.hidden_activation_derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.01      ,  0.01      ,  0.01      ],\n",
       "        [-1.02080529, -0.73480594,  1.48559353],\n",
       "        [-0.52143647, -1.36089508, -0.36556347]]),\n",
       " array([[ 0.01      ,  0.01      ],\n",
       "        [-1.25788996,  1.08418502],\n",
       "        [ 0.75540738,  0.68650945],\n",
       "        [ 0.60064027,  0.32373336]]),\n",
       " array([[ 0.        ],\n",
       "        [-1.01657088],\n",
       "        [-1.08121814]])]"
      ]
     },
     "execution_count": 743,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_mlp.initialize_weights()\n",
    "my_mlp.hidden_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[2, 2]]),\n",
       " array([[0.        , 0.        , 2.25006011]]),\n",
       " array([[1.36147672, 0.73841953]]),\n",
       " array([[-2.18243018]])]"
      ]
     },
     "execution_count": 752,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_mlp.hidden_activation_values"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "befc19283823365b4b77b90dae63e668f3a40f1a2491259fc4e466c5bacb6daf"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('ml-project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
