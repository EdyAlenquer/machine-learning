{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment 2\n",
    "## Logistic Regression and Stochastic Methods\n",
    "\n",
    "Aluno: Francisco Edyvalberty Alenquer Cordeiro \\\n",
    "Matrícula: 518659\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    right_prediction = y_true == y_pred\n",
    "    accuracy = right_prediction.sum() / len(y_true)\n",
    "    return accuracy\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    array = np.hstack([y_true.reshape(-1, 1), y_pred.reshape(-1, 1)])\n",
    "    array = array[array[:,0] == 1]\n",
    "    \n",
    "    right_prediction = array[:, 0] == array[:, 1]\n",
    "    recall = right_prediction.sum() / len(array)\n",
    "    return recall\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    array = np.hstack([y_true.reshape(-1, 1), y_pred.reshape(-1, 1)])\n",
    "    array = array[array[:,1] == 1]\n",
    "    \n",
    "    right_prediction = array[:, 0] == array[:, 1]\n",
    "    precision = right_prediction.sum() / len(array)\n",
    "\n",
    "    return precision\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    precision_score = precision(y_true, y_pred)\n",
    "    recall_score = recall(y_true, y_pred)\n",
    "\n",
    "    f1_score = 2 * (precision_score * recall_score) / (precision_score + recall_score)\n",
    "\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardScaler:\n",
    "    def __init__(self):\n",
    "        self.fitted = False\n",
    "\n",
    "    def fit_transform(self, data):      \n",
    "        self.mean = data.mean(axis=0)\n",
    "        self.std = data.std(axis=0)\n",
    "        self.fitted = True\n",
    "\n",
    "        scaled_data = (data - self.mean) / self.std\n",
    "        return scaled_data\n",
    "    \n",
    "    def transform(self, data):\n",
    "        if not self.fitted:\n",
    "            raise Exception('Scaler not fitted!')\n",
    "\n",
    "        scaled_data = (data - self.mean) / self.std\n",
    "        return scaled_data\n",
    "\n",
    "    def inverse_transform(self, scaled_data):\n",
    "        if not self.fitted:\n",
    "            raise Exception('Scaler not fitted!')\n",
    "        \n",
    "        original_data = (scaled_data * self.std) + self.mean\n",
    "        return original_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min-max feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinMaxScaler:\n",
    "    def __init__(self):\n",
    "        self.fitted = False\n",
    "\n",
    "    def fit_transform(self, data):      \n",
    "        self.maximum = data.max(axis=0)\n",
    "        self.minimum = data.min(axis=0)\n",
    "        self.fitted = True\n",
    "\n",
    "        scaled_data =  (data - self.minimum) / (self.maximum - self.minimum)\n",
    "        return scaled_data\n",
    "    \n",
    "    def transform(self, data):\n",
    "        if not self.fitted:\n",
    "            raise Exception('Scaler not fitted!')\n",
    "\n",
    "        scaled_data =  (data - self.minimum) / (self.maximum - self.minimum)\n",
    "        return scaled_data\n",
    "\n",
    "    def inverse_transform(self, scaled_data):\n",
    "        if not self.fitted:\n",
    "            raise Exception('Scaler not fitted!')\n",
    "        \n",
    "        original_data = (self.maximum - self.minimum) * scaled_data + self.minimum\n",
    "        return original_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfolds_cross_validation(data, n_folds=10, shuffle=False, random_state=12894):\n",
    "    indexes = np.arange(data.shape[0])\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.seed(12894)\n",
    "        np.random.shuffle(indexes)\n",
    "\n",
    "    slices = np.array_split(indexes, n_folds)\n",
    "    all_elements = np.hstack(slices)\n",
    "\n",
    "    for i in range(n_folds):\n",
    "        train_idx = all_elements[~np.isin(all_elements, slices[i])]\n",
    "        test_idx = slices[i]\n",
    "\n",
    "        yield train_idx, test_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data, train_size_perc, random_seed=264852):\n",
    "    N = data.shape[0]\n",
    "    train_size = int(train_size_perc * N)\n",
    "\n",
    "    indexes = np.arange(0, N, 1)\n",
    "\n",
    "    np.random.seed(random_seed)\n",
    "    train_idx = np.random.choice(indexes, train_size, replace=False)\n",
    "    test_idx = np.delete(indexes, train_idx)\n",
    "\n",
    "    train_data = data[train_idx]\n",
    "    test_data = data[test_idx]\n",
    "\n",
    "    X_train = train_data[:,:-1]\n",
    "    y_train = train_data[:,[-1]]\n",
    "\n",
    "    X_test = test_data[:,:-1]\n",
    "    y_test = test_data[:,[-1]]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (569, 31)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.799e+01, 1.038e+01, 1.228e+02, 1.001e+03, 1.184e-01, 2.776e-01,\n",
       "        3.001e-01, 1.471e-01, 2.419e-01, 7.871e-02, 1.095e+00, 9.053e-01,\n",
       "        8.589e+00, 1.534e+02, 6.399e-03, 4.904e-02, 5.373e-02, 1.587e-02,\n",
       "        3.003e-02, 6.193e-03, 2.538e+01, 1.733e+01, 1.846e+02, 2.019e+03,\n",
       "        1.622e-01, 6.656e-01, 7.119e-01, 2.654e-01, 4.601e-01, 1.189e-01,\n",
       "        0.000e+00],\n",
       "       [2.057e+01, 1.777e+01, 1.329e+02, 1.326e+03, 8.474e-02, 7.864e-02,\n",
       "        8.690e-02, 7.017e-02, 1.812e-01, 5.667e-02, 5.435e-01, 7.339e-01,\n",
       "        3.398e+00, 7.408e+01, 5.225e-03, 1.308e-02, 1.860e-02, 1.340e-02,\n",
       "        1.389e-02, 3.532e-03, 2.499e+01, 2.341e+01, 1.588e+02, 1.956e+03,\n",
       "        1.238e-01, 1.866e-01, 2.416e-01, 1.860e-01, 2.750e-01, 8.902e-02,\n",
       "        0.000e+00]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.genfromtxt('../data/breastcancer.csv', delimiter=',')\n",
    "print('Shape:', data.shape)\n",
    "data[:2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (455, 30)\n",
      "y_train shape: (455, 1)\n",
      "X_test shape: (114, 30)\n",
      "y_test shape: (114, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, 0.8, random_seed=12354)\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1+np.exp(-x))\n",
    "\n",
    "def cross_entropy_loss(y, y_pred_proba):\n",
    "    cost_1 = y.T @ np.log(y_pred_proba)\n",
    "    cost_0 = (1-y).T @ np.log(1-y_pred_proba)\n",
    "    j = -(1/len(y)) * (cost_1 + cost_0)\n",
    "    return j.ravel()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLogisticRegression():\n",
    "    def __init__(\n",
    "        self, \n",
    "        alpha, \n",
    "        n_iterations\n",
    "    ):        \n",
    "        self.alpha = alpha        \n",
    "        self.n_iterations = n_iterations\n",
    "\n",
    "    def initialize(self, X, y, random_state=654812):\n",
    "        rnd_state = np.random.RandomState(random_state)\n",
    "        self.X = np.hstack(\n",
    "            [np.ones((X.shape[0], 1)), X]\n",
    "        )\n",
    "        self.y = y\n",
    "\n",
    "        self.w = rnd_state.uniform(\n",
    "            0, \n",
    "            1, \n",
    "            self.X.shape[1]\n",
    "        ).reshape(-1, 1)\n",
    "\n",
    "    def fit(self, X, y, random_state=654812):\n",
    "        self.initialize(X, y, random_state)\n",
    "        self.gradient_descent()\n",
    "\n",
    "    def gradient_descent(self):\n",
    "        self.loss_by_iteration = []\n",
    "        for i in range(self.n_iterations):\n",
    "            actual_y_pred_proba = sigmoid(self.X @ self.w)\n",
    "            e = (self.y - actual_y_pred_proba) \n",
    "            self.w = self.w + ((1/len(self.y)) * self.alpha * (e.T @ self.X)).reshape(-1, 1)\n",
    "\n",
    "            new_y_pred_proba = sigmoid(self.X @ self.w)\n",
    "            self.loss_by_iteration.append(cross_entropy_loss(self.y, new_y_pred_proba))\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = np.hstack(\n",
    "            [np.ones((X.shape[0], 1)), X]\n",
    "        )\n",
    "        predict_proba = sigmoid(X @ self.w)\n",
    "        return predict_proba\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        X = np.hstack(\n",
    "            [np.ones((X.shape[0], 1)), X]\n",
    "        )\n",
    "        predict_proba = sigmoid(X @ self.w)\n",
    "        predict_label = np.where(predict_proba>threshold, 1, 0)\n",
    "        return predict_label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--->\tTraining Metrics\n",
      "Accuracy Mean:     \t0.9836 | Accuracy Std:   \t0.0019\n",
      "Recall Mean:     \t0.9928 | Recall Std:       \t0.0016\n",
      "Precision Mean:     \t0.9805 | Precision Std:   \t0.0021\n",
      "F1 Score Mean:     \t0.9866 | F1 Score Std:   \t0.0016\n",
      "\n",
      "--->\tValidation Metrics\n",
      "Accuracy Mean:     \t0.9758 | Accuracy Std:   \t0.0270\n",
      "Recall Mean:     \t0.9926 | Recall Std:       \t0.0148\n",
      "Precision Mean:     \t0.9691 | Precision Std:   \t0.0388\n",
      "F1 Score Mean:     \t0.9802 | F1 Score Std:   \t0.0216\n",
      "\n",
      "--->\tTest Metrics\n",
      "Accuracy:     \t0.9649\n",
      "Recall:     \t0.9877\n",
      "Precision:     \t0.9639\n",
      "F1 Score:     \t0.9756\n"
     ]
    }
   ],
   "source": [
    "train_metrics = {\n",
    "    'accuracy': [],\n",
    "    'recall': [],\n",
    "    'precision': [],\n",
    "    'f1_score': []\n",
    "}\n",
    "\n",
    "valid_metrics = {\n",
    "    'accuracy': [],\n",
    "    'recall': [],\n",
    "    'precision': [],\n",
    "    'f1_score': []\n",
    "}\n",
    "\n",
    "cv_splits = kfolds_cross_validation(\n",
    "    data=X_train,\n",
    "    n_folds=10,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "for train_idx, val_idx in cv_splits:\n",
    "    # Spliting data\n",
    "    X_train_cv = X_train[train_idx, :]\n",
    "    y_train_cv = y_train[train_idx, :]\n",
    "    X_val_cv = X_train[val_idx, :]\n",
    "    y_val_cv = y_train[val_idx, :]\n",
    "\n",
    "    # Scaling X_train\n",
    "    X_scaler = StandardScaler()\n",
    "    X_train_cv_scaled = X_scaler.fit_transform(X_train_cv)\n",
    "\n",
    "    # Training Model\n",
    "    my_lr = MyLogisticRegression(0.1, 500)\n",
    "    my_lr.fit(X_train_cv_scaled, y_train_cv)\n",
    "\n",
    "    # Scaling X_val based on X_train\n",
    "    X_val_cv_scaled = X_scaler.transform(X_val_cv)\n",
    "\n",
    "    # Predictions\n",
    "    y_train_cv_pred = my_lr.predict(X_train_cv_scaled)\n",
    "    y_val_cv_pred = my_lr.predict(X_val_cv_scaled)\n",
    "\n",
    "    # Storing metrics\n",
    "    train_metrics['accuracy'].append(accuracy(y_train_cv, y_train_cv_pred))\n",
    "    train_metrics['recall'].append(recall(y_train_cv, y_train_cv_pred))\n",
    "    train_metrics['precision'].append(precision(y_train_cv, y_train_cv_pred))\n",
    "    train_metrics['f1_score'].append(f1_score(y_train_cv, y_train_cv_pred))\n",
    "\n",
    "    valid_metrics['accuracy'].append(accuracy(y_val_cv, y_val_cv_pred))\n",
    "    valid_metrics['recall'].append(recall(y_val_cv, y_val_cv_pred))\n",
    "    valid_metrics['precision'].append(precision(y_val_cv, y_val_cv_pred))\n",
    "    valid_metrics['f1_score'].append(f1_score(y_val_cv, y_val_cv_pred))\n",
    "\n",
    "\n",
    "# Reporting results\n",
    "print('\\n--->\\tTraining Metrics')\n",
    "\n",
    "print('Accuracy Mean:     \\t{0:.4f} | Accuracy Std:   \\t{1:.4f}'.format(np.mean(train_metrics['accuracy']), np.std(train_metrics['accuracy'])))\n",
    "print('Recall Mean:     \\t{0:.4f} | Recall Std:       \\t{1:.4f}'.format(np.mean(train_metrics['recall']), np.std(train_metrics['recall'])))\n",
    "print('Precision Mean:     \\t{0:.4f} | Precision Std:   \\t{1:.4f}'.format(np.mean(train_metrics['precision']), np.std(train_metrics['precision'])))\n",
    "print('F1 Score Mean:     \\t{0:.4f} | F1 Score Std:   \\t{1:.4f}'.format(np.mean(train_metrics['f1_score']), np.std(train_metrics['f1_score'])))\n",
    "\n",
    "print('\\n--->\\tValidation Metrics')\n",
    "\n",
    "print('Accuracy Mean:     \\t{0:.4f} | Accuracy Std:   \\t{1:.4f}'.format(np.mean(valid_metrics['accuracy']), np.std(valid_metrics['accuracy'])))\n",
    "print('Recall Mean:     \\t{0:.4f} | Recall Std:       \\t{1:.4f}'.format(np.mean(valid_metrics['recall']), np.std(valid_metrics['recall'])))\n",
    "print('Precision Mean:     \\t{0:.4f} | Precision Std:   \\t{1:.4f}'.format(np.mean(valid_metrics['precision']), np.std(valid_metrics['precision'])))\n",
    "print('F1 Score Mean:     \\t{0:.4f} | F1 Score Std:   \\t{1:.4f}'.format(np.mean(valid_metrics['f1_score']), np.std(valid_metrics['f1_score'])))\n",
    "\n",
    "print('\\n--->\\tTest Metrics')\n",
    "\n",
    "# Scaling X_val based on X_train\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "y_test_pred = my_lr.predict(X_test_scaled).reshape(-1, 1)\n",
    "\n",
    "print('Accuracy:     \\t{0:.4f}'.format(accuracy(y_test, y_test_pred)))\n",
    "print('Recall:     \\t{0:.4f}'.format(recall(y_test, y_test_pred)))\n",
    "print('Precision:     \\t{0:.4f}'.format(precision(y_test, y_test_pred)))\n",
    "print('F1 Score:     \\t{0:.4f}'.format(f1_score(y_test, y_test_pred)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 - Gaussian Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (569, 31)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.799e+01, 1.038e+01, 1.228e+02, 1.001e+03, 1.184e-01, 2.776e-01,\n",
       "        3.001e-01, 1.471e-01, 2.419e-01, 7.871e-02, 1.095e+00, 9.053e-01,\n",
       "        8.589e+00, 1.534e+02, 6.399e-03, 4.904e-02, 5.373e-02, 1.587e-02,\n",
       "        3.003e-02, 6.193e-03, 2.538e+01, 1.733e+01, 1.846e+02, 2.019e+03,\n",
       "        1.622e-01, 6.656e-01, 7.119e-01, 2.654e-01, 4.601e-01, 1.189e-01,\n",
       "        0.000e+00],\n",
       "       [2.057e+01, 1.777e+01, 1.329e+02, 1.326e+03, 8.474e-02, 7.864e-02,\n",
       "        8.690e-02, 7.017e-02, 1.812e-01, 5.667e-02, 5.435e-01, 7.339e-01,\n",
       "        3.398e+00, 7.408e+01, 5.225e-03, 1.308e-02, 1.860e-02, 1.340e-02,\n",
       "        1.389e-02, 3.532e-03, 2.499e+01, 2.341e+01, 1.588e+02, 1.956e+03,\n",
       "        1.238e-01, 1.866e-01, 2.416e-01, 1.860e-01, 2.750e-01, 8.902e-02,\n",
       "        0.000e+00]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.genfromtxt('../data/breastcancer.csv', delimiter=',')\n",
    "print('Shape:', data.shape)\n",
    "data[:2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (455, 30)\n",
      "y_train shape: (455, 1)\n",
      "X_test shape: (114, 30)\n",
      "y_test shape: (114, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, 0.8, random_seed=646482)\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phi shape: (2, 1)\n",
      "Mu shape: (2, 30)\n",
      "Sigma shape: (2, 30, 30) \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class MyGaussianDiscriminantAnalysis():\n",
    "    def __init__(self):        \n",
    "        pass\n",
    "\n",
    "    def calculate_sigma(self, X, mu):\n",
    "        n_features = X.shape[1]\n",
    "        n_rows = X.shape[0]\n",
    "        sigma=np.zeros((n_features, n_features))\n",
    "\n",
    "        for i in range(n_rows):\n",
    "            x_i = X[i,:].reshape(n_features, 1)\n",
    "            sigma += (x_i-mu) @ (x_i-mu).T\n",
    "\n",
    "        return sigma/(n_rows-1)\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        classes = np.unique(y)\n",
    "        self.class_dict = {classes[i]: i for i in range(len(classes))}\n",
    "\n",
    "        n_features = X.shape[1]\n",
    "\n",
    "        self.phi = np.zeros((len(classes), 1)) # n_classes\n",
    "        self.mu = np.zeros((len(classes), n_features)) # n_classes x n_features\n",
    "        self.sigma = np.zeros((len(classes), n_features, n_features)) # n_classes x n_features\n",
    "\n",
    "        for label in classes:\n",
    "            \n",
    "            k = self.class_dict[label]\n",
    "\n",
    "            X_class = X[np.where(y==k)[0], :]\n",
    "            y_class = y[np.where(y==k)[0], :]\n",
    "            \n",
    "            self.phi[k] = len(y_class) / len(y)\n",
    "            self.mu[k] = np.mean(X_class, axis=0)\n",
    "            self.sigma[k] = self.calculate_sigma(X_class, self.mu[k].reshape(-1, 1))\n",
    "            # self.sigma[k] = self.calc_single_covariance(X, y)\n",
    "\n",
    "            \n",
    "        print('Phi shape:', self.phi.shape)\n",
    "        print('Mu shape:',self.mu.shape)\n",
    "        print('Sigma shape:',self.sigma.shape, '\\n\\n')\n",
    "            \n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        classes = list(self.class_dict.keys())\n",
    "        prob_classes = np.zeros((X.shape[0], len(classes)))\n",
    "        for i, label in enumerate(classes):\n",
    "\n",
    "            k = self.class_dict[label]\n",
    "            sigma_det = np.linalg.det(self.sigma[k])\n",
    "            sigma_inv = np.linalg.pinv(self.sigma[k])\n",
    "            mu = self.mu[[k]]\n",
    "\n",
    "            first_part = -(1/2)*np.log(sigma_det)\n",
    "            # TIRAR DUVIDA SOBRE ESSA OPERAÇÃO\n",
    "            second_part = -(1/2)*np.sum(((X-mu) @ sigma_inv) * (X-mu), axis=1)\n",
    "            third_part = np.log(self.phi[k])\n",
    "            \n",
    "            pred = first_part + second_part + third_part\n",
    "            prob_classes[:, i] = pred\n",
    "\n",
    "        preds = []\n",
    "        for i in range(prob_classes.shape[0]):\n",
    "            argmax = np.argmax(prob_classes[i, :])\n",
    "            preds.append(classes[argmax])\n",
    "            \n",
    "        return np.array(preds).reshape(-1, 1)\n",
    "        \n",
    "\n",
    "    def get_parameters(self):\n",
    "        print(self.phi)\n",
    "\n",
    "\n",
    "my_gda = MyGaussianDiscriminantAnalysis()\n",
    "my_gda.fit(X_train, y_train)\n",
    "preds = my_gda.predict(X_train[:, :])\n",
    "# my_gda.get_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.978021978021978\n",
      "0.9930313588850174\n",
      "0.9726962457337884\n",
      "0.9827586206896551\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(y_train, preds))\n",
    "print(recall(y_train, preds))\n",
    "print(precision(y_train, preds))\n",
    "print(f1_score(y_train, preds))\n",
    "# recall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = my_gda.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.956140350877193\n",
      "0.9714285714285714\n",
      "0.9577464788732394\n",
      "0.9645390070921985\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(y_test, preds))\n",
    "print(recall(y_test, preds))\n",
    "print(precision(y_test, preds))\n",
    "print(f1_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9648351648351648"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train, y_train.ravel())\n",
    "\n",
    "accuracy(y_train, lda.predict(X_train).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1.,\n",
       "        1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1.,\n",
       "        1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
       "        0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0.,\n",
       "        1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0.,\n",
       "        1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0.,\n",
       "        0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
       "        1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "        1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0.,\n",
       "        1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
       "        1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0.,\n",
       "        0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0.,\n",
       "        0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
       "        1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1.,\n",
       "        1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
       "        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1.,\n",
       "        0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1.,\n",
       "        0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
       "        1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
       "        0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1.,\n",
       "        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
       "        1., 1., 1., 1., 1., 1., 0.],\n",
       "       [1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0.,\n",
       "        1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.vstack([y_train.ravel(), np.array(preds)])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7d84fc3ab72c8387ddb373470e784917d8b759f8763a65d23fac12a2e8075760"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('ml-project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
